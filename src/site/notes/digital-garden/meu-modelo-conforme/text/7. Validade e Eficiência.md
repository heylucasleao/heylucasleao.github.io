---
{"dg-publish":true,"permalink":"/digital-garden/meu-modelo-conforme/text/7-validade-e-eficiencia/"}
---

# Validade e Eficiência

Ao tentar criar seu primeiro modelo de previsão conforme, você pode ter encontrado um desafio: muitos conjuntos de predição do seu modelo retorna todas as classes. Mas por que isso acontece?

À medida que aumentamos o nível de cobertura (por exemplo, para 99%), os conjuntos de previsão conforme tendem a crescer. Isso ocorre porque o modelo precisa incluir mais classes para garantir que a verdadeira classe esteja no conjunto de previsão com a probabilidade desejada.

Esse fenômeno está relacionado ao conceito de validade na previsão conforme. Ela assegura que a probabilidade de o conjunto de previsão conter a verdadeira classe seja, no mínimo, igual ao nível de confiança especificado. Por exemplo, com um nível de confiança de 99%, esperamos que o conjunto de previsão inclua a classe correta em pelo menos 99% dos casos.

Contudo, essa garantia pode resultar em conjuntos de previsão maiores, especialmente quando o nível de confiança é muito alto. Isso acontece porque o modelo precisa ser mais conservador em suas previsões para manter a garantia de cobertura.

Por outro lado, pode também ocorrer o inverso, aonde ambos os valores não atendem à cobertura, tornando vazio o conjunto de predições. Isso pode ocorrer pois estamos mexendo em um intervalo gerado a partir de diversos valores, não apenas um ponto fixo. Imagine que chegue um dado bem diferente do que já foi visto em todo histórico dos dados.

No artigo [Model-Agnostic Nonconformity Functions for Conformal Classification](https://ieeexplore.ieee.org/document/7966105), foram criadas duas métricas de não-conformidade, **AvgC** e **OneC**, retratando as propriedades de **validade **e **eficiência** da cobertura, que nos auxiliam bastante a entender como nosso modelo está performando. Validade refere-se à quantidade média de rótulos sinalizados pelo modelo por conjunto de predições, enquanto eficiência é a proporção de conjuntos de predições com apenas um valor. Quanto menor o valor de validade, mais informação podemos extrair do modelo, pois o conjunto de predições fica menor. Por outro lado, quanto maior o score OneC, melhor é a capacidade do modelo de extrair informação do score de não conformidade, conseguindo ser mais específico em seu resultado.

Podemos também ver as duas propriedades visualmente no link abaixo:

[Uncertainty Quantification (1): Enter Conformal Predictors](https://youtu.be/xZbuFKWV5NA?list=PLQkjeU22jN9tpbe7PpoA5nPG43SCgd7Sj&t=235)

Em meu projeto pessoal, fiz um gráfico que plota ambos os scores baseados em diferentes níveis de α. Caso tenha curiosidade, o código está disponível em meu GitHub. 

![image 6.png](/img/user/digital-garden/meu-modelo-conforme/assets/image%206.png)

Outras métricas que podem ser utilizadas para validação de modelo são:

## Total de conjunto Vazios

- Quantas vezes o modelo emitiu um conjunto de predições vazias, não sendo informativo em seu resultado.

## Taxa de Erro

- Média de conjunto de predições aonde o modelo errou em relação ao label original.

## Cobertura Empírica

É uma métrica que geramos para saber o quão temos a cobertura garantida no intervalo de predição em relação ao nível que foi modelado, em outras palavras, verificamos se, vamos supor, com um intervalo de 95% imposto à cobertura, temos aproximadamente 95% de garantia real sob ela. Caso ocorra de que esse valor seja relativamente inferior ao imposto, é possivel que o conceito de permutabilidade não esteja sendo atendida e toda modelagem precisa ser revisada!

Dentro dos meus modelos conformes no github há um código para geração dessa métrica, mas a lógica é o seguinte:

1. A partir de uma massa de dados, gero o score de não-conformidade e seto um número arbitrário na qual vou iterar sob eles.
2. A cada iteração, um percentual desses dados é selecionado de forma aleatória e posteriormente separados entre dado de calibração e teste, gerando o qhat baseado no alpha que modelei. Nesse caso, nosso alpha era 0.05. 
3. Calcula a média da cobertura dos dados selecionados para validação desta iteração.
4. Repetimos o processo até o número de iterações seja atingido.
5. Por fim tiramos a média de todas as coberturas geradas e comparamos com o valor modelado.

Há uma ótima explicação sobre isso no link abaixo:

[A Tutorial on Conformal Prediction Part 2: Conditional Coverage and Diagnostics](https://youtu.be/TRx4a2u-j7M?list=PLBa0oe-LYIHa68NOJbMxDTMMjT8Is4WkI&t=513)
