---
{"dg-publish":true,"permalink":"/digital-garden/meu-modelo-conforme/text/4-previsao-conforme/"}
---

# Previsão Conforme

A melhor descrição que encontrei sobre o tema encontra-se no artigo [Theoretical Foundations of Conformal Prediction](https://arxiv.org/abs/2411.11824), que descreve como:

> A previsão conforme é uma abordagem estatística para quantificação de incerteza onde as previsões do modelo são acompanhadas por um **intervalo **ou **conjunto**, comunicando o grau de confiabilidade em qualquer previsão, sem depender de pressupostos sobre a certeza do modelo.

Portanto, é um framework de aprendizagem de máquina que permite quantificar incertezas e criar intervalos de predição. Isso possibilita melhores interpretações do modelo, maior segurança e garantias de probabilidade empírica nos resultados.

Suas vantagens são:

- **Cobertura Garantida**: As regiões de previsão oferecem garantias de cobertura para o resultado final.
- **Agnosticismo de Modelo**: É compatível com qualquer modelo ou área de Machine Learning.
- **Independência de Distribuição**: As previsões conformes não pressupõem uma distribuição de probabilidade específica (Gaussiana, Gama, Poisson etc.), ampliando sua aplicabilidade.
- **Ausência de Retreino**: Não é necessário reestimar o modelo após a previsão inicial.
- **Predição Probabilística**: Oferece previsões com medidas de confiança, permitindo controlar o nível de garantia do próprio modelo.
- **Flexibilidade de Tamanho de Dados**: A validade das previsões é mantida independentemente do tamanho do conjunto de dados. Contudo, um conjunto de dados pequeno para calibração gera menos valor se comparado a uma volumetria maior.
- **Eficiência Computacional**: As previsões mantêm a eficiência computacional, sendo ideais para aplicações em tempo real e grandes conjuntos de dados.

Existem dois tipos de categorias: Transductive Conformal Prediction (TCP) e Inductive Conformal Prediction (ICP). 

**Transductive Conformal Prediction (TCP):**
O TCP foi o método **original** de Previsão Conforme. Sua característica principal é que, para cada novo ponto de dados, o modelo é treinado inteiramente para **cada** resultado possível. Por exemplo, em um modelo binário de classificação, o modelo será treinado uma vez para o valor 0 e outra para o valor 1, calculando um score de não conformidade para cada caso. 

> [!tip] 
> Ou seja, o modelo irá ser treinado para cada rótulo e para cada ponto novo a ser testado!

Esse processo garante alta precisão, pois considera toda a informação disponível para cada previsão. Este processo se assimila na técnica Jacknife, que faz uma é um processo de leave-one-out para estimação de variância e viés , aonde:

1. Remove de forma **não **aleatória uma observação por vez do conjunto de dados
2. Calcula a estatística de interesse com as observações restantes
3. Utiliza essas estimativas para avaliar o variância e viés do estimador

No contexto da Previsão Conforme, esta abordagem tem como objetivo as estimativas de incerteza, embora⁠, em contra partida pode ser computacionalmente ineficiente dependendo da quantidade de dados tanto de treinamento, como para teste. 

**Inductive Conformal Prediction (ICP):**
O ICP foi desenvolvido como uma alternativa mais eficiente ao TCP. Nele treinamos uma única vez utilizando dados **separados** ao do treinamento do nosso modelo base. Com esse conjunto de dados de calibração, geramos um intervalo que são aplicados a novos dados para previsão.

Embora o TCP ofereça potencialmente maior precisão, o ICP é geralmente preferido em aplicações práticas devido à sua eficiência computacional, especialmente em cenários com a necessidade de previsões em tempo real.

Uma explicação visual sobre a diferença de ambos pode ser vista por aqui:

[Uncertainty Quantification (3): From Full to Split Conformal Methods](https://youtu.be/YigGJfsCjDk?si=GdeR7xnGCZCZeOhk)

Logo, para este projeto, farei apenas o método ICP.

## Cobertura Garantida

Para compreender a garantia de cobertura, vamos analisar duas expressões matemáticas fundamentais:

$$
P(Y_{n+1} \in C(X_{n+1})) \geq 1 - \alpha
$$

Ela que a probabilidade do próximo valor Y estar contido no conjunto de predição C(Xn+1) é de pelo menos 1-α, onde α representa nossa taxa de erro desejada.

$$
C(X_{n+1}) = \{y \in Y : s(X_{n+1}, y) \leq \hat{q}\}
$$

Esta segunda expressão define o conjunto de predição utilizando um score de não-conformidade. Este score, calculado a partir de novos dados, é comparado com o quantil conforme (q̂), que estabelece nosso limiar para dados conformes. Em termos práticos, isso cria um conjunto de predição onde temos uma garantia probabilística de 1-α de que o próximo valor Y estará contido.

Para que isso seja respeitado, devemos assumir que os dados respeitem a permutabilidase e que sejam dados independentes e identicamente distribuídos (i.i.d.), assegurando a validade estatística da cobertura.

## Permutabilidade

O termo se refere à propriedade de uma sequência de variáveis aleatórias em que a ordem de aparecimento não afeta sua distribuição de probabilidade conjunta.

Em termos mais simples, significa que os elementos de uma sequência podem ser reorganizados sem afetar suas propriedades probabilísticas fundamentais. Por exemplo:

Imagine uma urna com 3 bolas coloridas (vermelha, azul e verde) das quais você vai retirar uma a uma. Considere as seguintes sequências possíveis:

- Sequência 1: (Vermelha, Azul, Verde)
- Sequência 2: (Verde, Vermelha, Azul)
- Sequência 3: (Azul, Verde, Vermelha)

Se as bolas são retiradas aleatoriamente e recolocadas, a probabilidade de obter qualquer uma dessas sequências é idêntica. Isso demonstra que a sequência é permutável — você pode reorganizar os elementos e a distribuição de probabilidade permanece inalterada.

Em resumo, para que a previsão conforme funcione adequadamente, os dados de calibração **devem** seguir a mesma distribuição dos dados que serão preditos no dia a dia. Podemos validar isso através de diferentes métodos: análise de cobertura empírica, que abordarei posteriormente, teste t de Student ou teste de Kolmogorov-Smirnov.

