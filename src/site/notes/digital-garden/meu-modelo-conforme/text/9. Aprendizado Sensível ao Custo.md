---
{"dg-publish":true,"permalink":"/digital-garden/meu-modelo-conforme/text/9-aprendizado-sensivel-ao-custo/"}
---

# Aprendizado Sens√≠vel ao Custo

O Aprendizado Sens√≠vel ao Custo √© uma abordagem que busca otimizar decis√µes considerando os custos associados a diferentes tipos de erros. Diferentemente das abordagens tradicionais, onde os custos de erro s√£o sim√©tricos, esta abordagem lida com custos assim√©tricos ‚Äî alguns erros podem ser mais custosos ou prejudiciais que outros. Esses erros podem ser de predi√ß√£o (Falsos Positivos ou Negativos) ou relacionados a regras de neg√≥cio, como fraudes ou aprova√ß√£o de um servi√ßo caro. A ideia central √© minimizar o custo total das previs√µes, em vez de simplesmente maximizar a precis√£o.

Exemplo de Treinamento com Aprendizado Sens√≠vel ao Custo:

Considere um banco desenvolvendo um modelo para detectar transa√ß√µes fraudulentas. A matriz de custos poderia ser estruturada da seguinte forma:

| Previs√£o \ Real | Fraude | N√£o Fraude |
| --- | --- | --- |
| Fraude | $0 | $50 |
| N√£o Fraude | $1000 | $0 |

Neste cen√°rio:

- Falso Positivo (classificar uma transa√ß√£o leg√≠tima como fraude): Custa $50 √† empresa devido √† insatisfa√ß√£o do cliente.
- Falso Negativo (n√£o detectar uma fraude real): Custa $1000 √† empresa, representando o valor m√©dio de uma transa√ß√£o fraudulenta.
- Verdadeiro Positivo e Verdadeiro Negativo: N√£o t√™m custos associados.

Logo, o modelo treinado ajustaria seus par√¢metros para minimizar o custo total. Isso pode resultar em aceitar mais falsos positivos para reduzir significativamente os falsos negativos mais custosos. Embora possa ter uma precis√£o geral ligeiramente menor, esse modelo seria mais eficaz em termos de redu√ß√£o de perdas financeiras para a empresa.

## Dados Desbalanceados

Al√©m disso, podemos aplicar essa metodologia para lidar com conjuntos de dados desbalanceados, onde uma classe √© significativamente mais representada que outra. Atribu√≠mos custos mais altos aos erros da classe menos representada, controlando assim a confiabilidade do modelo em rela√ß√£o √†s classes.

Para implementar isso, primeiro criamos um peso para cada classe, proporcional ao volume de dados para treino. Simulando para duas classes:

$$
\text{peso-da-classe} = \frac{\text{total-de-dados}}{\text{total-de-classes} \times \text{total-de-registros-da-classe-espec√≠fica}}



$$

```python
total_de_dados = 100
total_da_classe_0 = len(y_train[y_train == 0]) # 90
w1 = 100 / (2 * total_da_classe_0) #0.55

total_da_classe_1 = len(y_train[y_train == 1]) # 10
w2 = 100 / (2 * total_da_classe_1) #5

rf = RandomForestClassifier()
rf.fit(X_train, y_train, class_weight={0: 0.55, 1: 5})
```

Desta forma, contabilizar√° os pesos das duas classes dentro da fun√ß√£o de perda. Neste caso, dentro de gini, contabilizar√° os dois pesos.

$$
\text{Gini} = 1 - \text{W}_1*\text{Proportion}_{c1}^2 - \text{W}_2*\text{Proportion}_{c2}^2
$$

scikit-learn por padr√£o, possibilita j√° calcular esse peso, apenas incrementado o parametro como ‚Äúbalanced‚Äù.

```python
rf.fit(X_train, y_train, class_weight="balanced")
```

H√° outros m√©todos utilizando metadados mais espec√≠ficos. Para mais informa√ß√µes, recomendo a palestra no EuroSciPy 2023.

[EuroSciPy 2023 - Get the best from your scikit-learn classifier](https://www.youtube.com/watch?v=6YnhoCfArQo&t=8s)

## Avalia√ß√£o de Performance

Precisamos observar corretamente como nosso modelo atua no cen√°rio do nosso problema. Para isso, √© importante selecionar a melhor m√©trica para avalia√ß√£o. Existe uma lista expressiva de op√ß√µes. No contexto de classifica√ß√£o bin√°ria, irei abordar apenas as m√©tricas que s√£o de meu interesse.

√â importante destacar que todas essas m√©tricas derivam da matriz de confus√£o, utilizando conceitos como Verdadeiro Positivo (VP), Falso Positivo (FP), Verdadeiro Negativo (VN) e Falso Negativo (FN).

> [!tip] üí°
> Sempre que precisar discutir ou revisar alguma m√©trica para adequ√°-la ao seu problema, sugiro fortemente consultar o [guia](https://github.com/NannyML/The-Little-Book-of-ML-Metrics) disponibilizado pela Nannyml ‚Äî vale a pena!

## F1-Score

F1-Score √© uma das m√©tricas mais utilizadas para avaliar modelos. Seu c√°lculo √© feito pela a m√©dia harm√¥nica entre precis√£o e sensibilidade, fornecendo um √∫nico valor que equilibra ambas as m√©tricas. Sua [f√≥rmula](https://youtu.be/GCPDWXWN55U?si=Q4xZMQxgRUlrlcTw) √©:

$$
F1 = 2 \times \frac{\text{Precis√£o} \times \text{Sensibilidade}}{\text{Precis√£o} + \text{Sensibilidade}}
$$

Onde:

- Precis√£o = VP / (VP + FP)
- Sensibilidade = VP / (VP + FN)

√â particularmente √∫til quando buscamos um equil√≠brio entre precis√£o e sensibilidade. Um valor alto indica que o modelo possui tanto boa precis√£o quanto boa sensibilidade.

Entretanto, o F1-Score n√£o contabiliza os verdadeiros negativos e d√° √™nfase √† classe positiva, o que o torna a ser enviesado. Para entendermos isso, precisamos voltar e falar de como ele surgiu e qual era o seu prop√≥sito inicial. O F1-Score foi desenvolvido por van Rijsbergen, em 1979, para avaliar sistemas de recupera√ß√£o de informa√ß√£o, onde a precis√£o e sensibilidade tem maior grau de import√¢ncia do que a predi√ß√£o correta de VN. Segundo o artigo [Assessing Software Defection Prediction Performance: Why Using the Matthews Correlation Coefficient Matters](https://arxiv.org/pdf/2003.01182), √© exemplificado isso:

> Este dom√≠nio do problema √© caracterizado por contagens muito grandes, e frequentemente desconhecidas, de VN. Considere, por exemplo, a recupera√ß√£o de p√°ginas web. Saber o n√∫mero de p√°ginas irrelevantes corretamente n√£o recuperadas, ou seja, verdadeiros negativos, ser√° tanto desafiador quanto pouco interessante. Isso chegaria a centenas de milh√µes, possivelmente mais. Infelizmente, aplicar F1 no contexto completamente diferente de predi√ß√£o de defeitos n√£o √© equivalente.

Isso √© bastante percept√≠vel quando h√° um grau de import√¢ncia nos Verdadeiros Negativos (VN) ou quando existe um desbalanceamento para a classe negativa. Nessas condi√ß√µes, a m√©trica pode mascarar a real performance do modelo. Tal fato √© evidenciado tamb√©m no artigo [The advantages of the Matthews correlation coefficient (MCC) over F1 scoreand accuracy in binary classification evaluation](https://link.springer.com/article/10.1186/s12864-019-6413-7), que demonstra cen√°rios onde a m√©trica pode gerar uma confian√ßa equivocada no modelo, sugerindo o uso de m√©tricas alternativas que sejam mais representativas do contexto geral.

Por exemplo, supondo que temos um modelo de detec√ß√£o de indicativos de um paciente para uma doen√ßa muito comum: 

- Dados de Teste: 1000 pacientes
- 980 pacientes (98%) t√™m a doen√ßa (positivos)
- 20 pacientes (2%) n√£o t√™m a doen√ßa (negativos)

Se um modelo simplesmente classificar todos como positivos:

- F1-Score: 0.99

Com essa m√©trica, n√£o conseguimos avaliar se o modelo est√° superestimando os resultados ou simplesmente classificando todos os pacientes como doentes, mascarando assim problemas mais s√©rios. Por isso, recomenda-se utilizar m√©tricas que ofere√ßam um contexto mais amplo e uma vis√£o mais completa, considerando o peso de toda a matriz de confus√£o. Assim, abordarei a acur√°cia balanceada, uma normaliza√ß√£o da m√©trica ***Bookmaker informedness*** (***BM***).

> [!tip]
> N√£o h√° problema em usar o F1-Score, por√©m √© recomend√°vel combin√°-lo com outra m√©trica para obter um contexto maior do modelo.

### Acur√°cia Balanceada

A partir delas podemos derivar uma m√©trica mais robusta para dados desbalanceados: a **acur√°cia balanceada (BA)**. Seu diferencial √© avaliar o desempenho do modelo considerando igualmente todas as classes, independentemente de sua frequ√™ncia nos dados.

A acur√°cia balanceada √© calculada como a m√©dia aritm√©tica entre **sensibilidade** e **especificidade**. Para um problema de classifica√ß√£o bin√°ria, a f√≥rmula √©:

$$
\text{Acur√°cia Balanceada} = \frac{1}{2} \left(\frac{\text{VP}}{\text{VP} + \text{FN}} + \frac{\text{VN}}{\text{VN} + \text{FP}}\right)
$$

Onde:

A acur√°cia balanceada √© √∫til em cen√°rios onde as classes t√™m igual relev√¢ncia, independentemente de sua distribui√ß√£o nos dados. 

Por exemplo, imagine um sistema de controle de qualidade em uma f√°brica de componentes cr√≠ticos de aeronaves:

- Volumetria: 1.000 pe√ßas
- 950 (95%) das pe√ßas s√£o aprovadas no controle de qualidade
- 5 (5%) das pe√ßas apresentam defeitos

Se um modelo de detec√ß√£o autom√°tica sempre aprovar as pe√ßas, ter√≠amos:

- Acur√°cia normal: 95%
- Acur√°cia balanceada: 50%

Neste caso, tanto aprovar uma pe√ßa defeituosa quanto rejeitar uma pe√ßa boa s√£o igualmente cr√≠ticos: o primeiro pode comprometer a seguran√ßa da aeronave, e o segundo representa um preju√≠zo financeiro significativo, pois s√£o pe√ßas caras! Neste contexto em que os tipos de erros s√£o igualmente importantes, √© interessante considerarmos a acur√°cia balanceada.

Retornando ao contexto original do problema ‚Äî adaptar o modelo para dados possivelmente desbalanceados ‚Äî e considerando que ambos os erros de classifica√ß√£o t√™m igual import√¢ncia, a **acur√°cia balanceada** √© mais adequada.

# Atualiza√ß√£o de Projeto!

Ap√≥s revis√£o do meu projeto, resolvi fazer uma altera√ß√£o na vers√£o da [biblioteca](https://github.com/HeyLucasLeao/tinycp/tree/main). Nela, utilizo a possibilidade de escolher entre duas m√©tricas, Bookmaker Informedness (BM) ou Coeficiente de Correla√ß√£o de Matthews (MCC). 

### Bookmaker Informedness

O Bookmaker Informedness √© outra m√©trica importante para avalia√ß√£o de modelos, especialmente em cen√°rios com classes desbalanceadas. Assim como a acur√°cia balanceada, ela busca fornecer uma avalia√ß√£o mais robusta do desempenho do modelo. Na realidade, BA √© uma normaliza√ß√£o de BM.

O BM √© calculado como:

$$
\text{BM} = \text{Sensibilidade} + \text{Especificidade} - 1
$$

Esta m√©trica:

- Varia de -1 a +1, onde +1 indica predi√ß√£o perfeita
- 0 indica que o modelo n√£o √© melhor que escolhas aleat√≥rias
- Valores negativos indicam desempenho pior que aleat√≥rio

O Bookmaker Informedness √© particularmente √∫til quando as classes t√™m igual import√¢ncia, independente de sua distribui√ß√£o nos dados, como tamb√©m √© a √∫nica m√©trica para avaliar aleatoriedade da predi√ß√£o, demonstrado no artigo [The Matthews correlation coefficient (MCC) is more reliable than balanced accuracy, bookmaker informedness, and markedness in two-class confusion matrix evaluation - BioData Mining](https://biodatamining.biomedcentral.com/articles/10.1186/s13040-021-00244-z)

### Coeficiente de Correla√ß√£o de Matthews

O MCC √© considerado uma das m√©tricas mais completas para avalia√ß√£o de classificadores bin√°rios, pois:

- Considera todos os elementos da matriz de confus√£o (VP, VN, FP, FN)
- √â especialmente √∫til para classes desbalanceadas
- Fornece uma medida mais confi√°vel mesmo quando as classes t√™m tamanhos muito diferentes
- Em caso de seu score alto, m√©tricas como Brier Score, F1-Score, BM, ROC-AUC costumam apresentar bons resultados.

A f√≥rmula do MCC √©:

$$
MCC = \frac{VP \times VN - FP \times FN}{\sqrt{(VP + FP)(VP + FN)(VN + FP)(VN + FN)}}
$$

O MCC tem as seguintes caracter√≠sticas:

- Varia de -1 a +1, similar ao Bookmaker Informedness
- +1 representa uma classifica√ß√£o perfeita
- 0 indica desempenho equivalente a predi√ß√µes aleat√≥rias
- -1 indica o pior desempenho poss√≠vel

O MCC √© particularmente √∫til quando as classes t√™m igual import√¢ncia, independentemente de sua distribui√ß√£o nos dados. Diferentemente de outras m√©tricas, existe uma extensa literatura que recomenda seu uso como padr√£o no campo da estat√≠stica para garantir maior rigor na avalia√ß√£o dos modelos.

Portanto, optou-se por utilizar tanto o MCC quanto o BM para avaliar os resultados.

> [!tip] 
> Como o MCC considera tanto a preval√™ncia dos dados positivos quanto o vi√©s de qu√£o prov√°vel o modelo prev√™ corretamente a base de dados, n√£o √© aconselh√°vel utiliz√°-lo quando queremos comparar resultados entre diferentes bases de dados.
> Para esse cen√°rio, √© melhor utilizar BA ou BM.

## Aprendizado Sens√≠vel a Custo versus Reamostragem

Uma diferen√ßa crucial √© que, ao contr√°rio dos m√©todos de undersampling ou oversampling, n√£o modificamos a distribui√ß√£o dos dados durante o treinamento. O modelo √© treinado com a distribui√ß√£o real dos dados dispon√≠veis, alterando apenas a forma como aprende a partir deles. Isso traz uma vantagem especial quando precisamos estimar a precis√£o e probabilidade do modelo em um cen√°rio real, pois a distribui√ß√£o dos dados reflete fielmente a realidade.
