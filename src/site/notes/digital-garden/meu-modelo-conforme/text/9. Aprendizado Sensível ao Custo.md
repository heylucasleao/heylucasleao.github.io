---
{"dg-publish":true,"permalink":"/digital-garden/meu-modelo-conforme/text/9-aprendizado-sensivel-ao-custo/"}
---

# Aprendizado Sensível ao Custo

O Aprendizado Sensível ao Custo é uma abordagem que busca otimizar decisões considerando os custos associados a diferentes tipos de erros. Diferentemente das abordagens tradicionais, onde os custos de erro são simétricos, esta abordagem lida com custos assimétricos — alguns erros podem ser mais custosos ou prejudiciais que outros. Esses erros podem ser de predição (Falsos Positivos ou Negativos) ou relacionados a regras de negócio, como fraudes ou aprovação de um serviço caro. A ideia central é minimizar o custo total das previsões, em vez de simplesmente maximizar a precisão.

Exemplo de Treinamento com Aprendizado Sensível ao Custo:

Considere um banco desenvolvendo um modelo para detectar transações fraudulentas. A matriz de custos poderia ser estruturada da seguinte forma:

| Previsão \ Real | Fraude | Não Fraude |
| --- | --- | --- |
| Fraude | $0 | $50 |
| Não Fraude | $1000 | $0 |

Neste cenário:

- Falso Positivo (classificar uma transação legítima como fraude): Custa $50 à empresa devido à insatisfação do cliente.
- Falso Negativo (não detectar uma fraude real): Custa $1000 à empresa, representando o valor médio de uma transação fraudulenta.
- Verdadeiro Positivo e Verdadeiro Negativo: Não têm custos associados.

Logo, o modelo treinado ajustaria seus parâmetros para minimizar o custo total. Isso pode resultar em aceitar mais falsos positivos para reduzir significativamente os falsos negativos mais custosos. Embora possa ter uma precisão geral ligeiramente menor, esse modelo seria mais eficaz em termos de redução de perdas financeiras para a empresa.

## Dados Desbalanceados

Além disso, podemos aplicar essa metodologia para lidar com conjuntos de dados desbalanceados, onde uma classe é significativamente mais representada que outra. Atribuímos custos mais altos aos erros da classe menos representada, controlando assim a confiabilidade do modelo em relação às classes.

Para implementar isso, primeiro criamos um peso para cada classe, proporcional ao volume de dados para treino. Simulando para duas classes:

$$
\text{peso-da-classe} = \frac{\text{total-de-dados}}{\text{total-de-classes} \times \text{total-de-registros-da-classe-específica}}



$$

```python
total_de_dados = 100
total_da_classe_0 = len(y_train[y_train == 0]) # 90
w1 = 100 / (2 * total_da_classe_0) #0.55

total_da_classe_1 = len(y_train[y_train == 1]) # 10
w2 = 100 / (2 * total_da_classe_1) #5

rf = RandomForestClassifier()
rf.fit(X_train, y_train, class_weight={0: 0.55, 1: 5})
```

Desta forma, contabilizará os pesos das duas classes dentro da função de perda. Neste caso, dentro de gini, contabilizará os dois pesos.

$$
\text{Gini} = 1 - \text{W}_1*\text{Proportion}_{c1}^2 - \text{W}_2*\text{Proportion}_{c2}^2
$$

scikit-learn por padrão, possibilita já calcular esse peso, apenas incrementado o parametro como “balanced”.

```python
rf.fit(X_train, y_train, class_weight="balanced")
```

Há outros métodos utilizando metadados mais específicos. Para mais informações, recomendo a palestra no EuroSciPy 2023.

[EuroSciPy 2023 - Get the best from your scikit-learn classifier](https://www.youtube.com/watch?v=6YnhoCfArQo&t=8s)

## Avaliação de Performance

Precisamos observar corretamente como nosso modelo atua no cenário do nosso problema. Para isso, é importante selecionar a melhor métrica para avaliação. Existe uma lista expressiva de opções. No contexto de classificação binária, irei abordar apenas as métricas que são de meu interesse.

É importante destacar que todas essas métricas derivam da matriz de confusão, utilizando conceitos como Verdadeiro Positivo (VP), Falso Positivo (FP), Verdadeiro Negativo (VN) e Falso Negativo (FN).

> [!tip] 
> Sempre que precisar discutir ou revisar alguma métrica para adequá-la ao seu problema, sugiro fortemente consultar o [guia](https://github.com/NannyML/The-Little-Book-of-ML-Metrics) disponibilizado pela Nannyml — vale a pena!

## F1-Score

F1-Score é uma das métricas mais utilizadas para avaliar modelos. Seu cálculo é feito pela a média harmônica entre precisão e sensibilidade, fornecendo um único valor que equilibra ambas as métricas. Sua [fórmula](https://youtu.be/GCPDWXWN55U?si=Q4xZMQxgRUlrlcTw) é:

$$
F1 = 2 \times \frac{\text{Precisão} \times \text{Sensibilidade}}{\text{Precisão} + \text{Sensibilidade}}
$$

Onde:

- Precisão = VP / (VP + FP)
- Sensibilidade = VP / (VP + FN)

É particularmente útil quando buscamos um equilíbrio entre precisão e sensibilidade. Um valor alto indica que o modelo possui tanto boa precisão quanto boa sensibilidade.

Entretanto, o F1-Score não contabiliza os verdadeiros negativos e dá ênfase à classe positiva, o que o torna a ser enviesado. Para entendermos isso, precisamos voltar e falar de como ele surgiu e qual era o seu propósito inicial. O F1-Score foi desenvolvido por van Rijsbergen, em 1979, para avaliar sistemas de recuperação de informação, onde a precisão e sensibilidade tem maior grau de importância do que a predição correta de VN. Segundo o artigo [Assessing Software Defection Prediction Performance: Why Using the Matthews Correlation Coefficient Matters](https://arxiv.org/pdf/2003.01182), é exemplificado isso:

> Este domínio do problema é caracterizado por contagens muito grandes, e frequentemente desconhecidas, de VN. Considere, por exemplo, a recuperação de páginas web. Saber o número de páginas irrelevantes corretamente não recuperadas, ou seja, verdadeiros negativos, será tanto desafiador quanto pouco interessante. Isso chegaria a centenas de milhões, possivelmente mais. Infelizmente, aplicar F1 no contexto completamente diferente de predição de defeitos não é equivalente.

Isso é bastante perceptível quando há um grau de importância nos Verdadeiros Negativos (VN) ou quando existe um desbalanceamento para a classe negativa. Nessas condições, a métrica pode mascarar a real performance do modelo. Tal fato é evidenciado também no artigo [The advantages of the Matthews correlation coefficient (MCC) over F1 scoreand accuracy in binary classification evaluation](https://link.springer.com/article/10.1186/s12864-019-6413-7), que demonstra cenários onde a métrica pode gerar uma confiança equivocada no modelo, sugerindo o uso de métricas alternativas que sejam mais representativas do contexto geral.

Por exemplo, supondo que temos um modelo de detecção de indicativos de um paciente para uma doença muito comum: 

- Dados de Teste: 1000 pacientes
- 980 pacientes (98%) têm a doença (positivos)
- 20 pacientes (2%) não têm a doença (negativos)

Se um modelo simplesmente classificar todos como positivos:

- F1-Score: 0.99

Com essa métrica, não conseguimos avaliar se o modelo está superestimando os resultados ou simplesmente classificando todos os pacientes como doentes, mascarando assim problemas mais sérios. Por isso, recomenda-se utilizar métricas que ofereçam um contexto mais amplo e uma visão mais completa, considerando o peso de toda a matriz de confusão. Assim, abordarei a acurácia balanceada, uma normalização da métrica ***Bookmaker informedness*** (***BM***).

> [!tip]
> Não há problema em usar o F1-Score, porém é recomendável combiná-lo com outra métrica para obter um contexto maior do modelo.

### Acurácia Balanceada

A partir delas podemos derivar uma métrica mais robusta para dados desbalanceados: a **acurácia balanceada (BA)**. Seu diferencial é avaliar o desempenho do modelo considerando igualmente todas as classes, independentemente de sua frequência nos dados.

A acurácia balanceada é calculada como a média aritmética entre **sensibilidade** e **especificidade**. Para um problema de classificação binária, a fórmula é:

$$
\text{Acurácia Balanceada} = \frac{1}{2} \left(\frac{\text{VP}}{\text{VP} + \text{FN}} + \frac{\text{VN}}{\text{VN} + \text{FP}}\right)
$$

Onde:

A acurácia balanceada é útil em cenários onde as classes têm igual relevância, independentemente de sua distribuição nos dados. 

Por exemplo, imagine um sistema de controle de qualidade em uma fábrica de componentes críticos de aeronaves:

- Volumetria: 1.000 peças
- 950 (95%) das peças são aprovadas no controle de qualidade
- 5 (5%) das peças apresentam defeitos

Se um modelo de detecção automática sempre aprovar as peças, teríamos:

- Acurácia normal: 95%
- Acurácia balanceada: 50%

Neste caso, tanto aprovar uma peça defeituosa quanto rejeitar uma peça boa são igualmente críticos: o primeiro pode comprometer a segurança da aeronave, e o segundo representa um prejuízo financeiro significativo, pois são peças caras! Neste contexto em que os tipos de erros são igualmente importantes, é interessante considerarmos a acurácia balanceada.

Retornando ao contexto original do problema — adaptar o modelo para dados possivelmente desbalanceados — e considerando que ambos os erros de classificação têm igual importância, a **acurácia balanceada** é mais adequada.

# Atualização de Projeto!

Após revisão do meu projeto, resolvi fazer uma alteração na versão da [biblioteca](https://github.com/HeyLucasLeao/tinycp/tree/main). Nela, utilizo a possibilidade de escolher entre duas métricas, Bookmaker Informedness (BM) ou Coeficiente de Correlação de Matthews (MCC). 

### Bookmaker Informedness

O Bookmaker Informedness é outra métrica importante para avaliação de modelos, especialmente em cenários com classes desbalanceadas. Assim como a acurácia balanceada, ela busca fornecer uma avaliação mais robusta do desempenho do modelo. Na realidade, BA é uma normalização de BM.

O BM é calculado como:

$$
\text{BM} = \text{Sensibilidade} + \text{Especificidade} - 1
$$

Esta métrica:

- Varia de -1 a +1, onde +1 indica predição perfeita
- 0 indica que o modelo não é melhor que escolhas aleatórias
- Valores negativos indicam desempenho pior que aleatório

O Bookmaker Informedness é particularmente útil quando as classes têm igual importância, independente de sua distribuição nos dados, como também é a única métrica para avaliar aleatoriedade da predição, demonstrado no artigo [The Matthews correlation coefficient (MCC) is more reliable than balanced accuracy, bookmaker informedness, and markedness in two-class confusion matrix evaluation - BioData Mining](https://biodatamining.biomedcentral.com/articles/10.1186/s13040-021-00244-z)

### Coeficiente de Correlação de Matthews

O MCC é considerado uma das métricas mais completas para avaliação de classificadores binários, pois:

- Considera todos os elementos da matriz de confusão (VP, VN, FP, FN)
- É especialmente útil para classes desbalanceadas
- Fornece uma medida mais confiável mesmo quando as classes têm tamanhos muito diferentes
- Em caso de seu score alto, métricas como Brier Score, F1-Score, BM, ROC-AUC costumam apresentar bons resultados.

A fórmula do MCC é:

$$
MCC = \frac{VP \times VN - FP \times FN}{\sqrt{(VP + FP)(VP + FN)(VN + FP)(VN + FN)}}
$$

O MCC tem as seguintes características:

- Varia de -1 a +1, similar ao Bookmaker Informedness
- +1 representa uma classificação perfeita
- 0 indica desempenho equivalente a predições aleatórias
- -1 indica o pior desempenho possível

O MCC é particularmente útil quando as classes têm igual importância, independentemente de sua distribuição nos dados. Diferentemente de outras métricas, existe uma extensa literatura que recomenda seu uso como padrão no campo da estatística para garantir maior rigor na avaliação dos modelos.

Portanto, optou-se por utilizar tanto o MCC quanto o BM para avaliar os resultados.

> [!tip] 
> Como o MCC considera tanto a prevalência dos dados positivos quanto o viés de quão provável o modelo prevê corretamente a base de dados, não é aconselhável utilizá-lo quando queremos comparar resultados entre diferentes bases de dados.
> Para esse cenário, é melhor utilizar BA ou BM.

## Aprendizado Sensível a Custo versus Reamostragem

Uma diferença crucial é que, ao contrário dos métodos de undersampling ou oversampling, não modificamos a distribuição dos dados durante o treinamento. O modelo é treinado com a distribuição real dos dados disponíveis, alterando apenas a forma como aprende a partir deles. Isso traz uma vantagem especial quando precisamos estimar a precisão e probabilidade do modelo em um cenário real, pois a distribuição dos dados reflete fielmente a realidade.
