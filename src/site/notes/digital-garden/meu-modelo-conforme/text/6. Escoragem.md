---
{"dg-publish":true,"permalink":"/digital-garden/meu-modelo-conforme/text/6-escoragem/"}
---

# Escoragem

Anteriormente, geramos a cobertura para os conjuntos de previsão. Na verdade, o método que utilizamos é o mais tradicional, conhecido como cobertura **marginal**. Ele é excelente para situações em que se deseja atender à distribuição como um todo. Porém, dependendo do seu problema, sua adaptabilidade — ou seja, a capacidade de atender às classes específicas — pode não ser ideal. Não há certo ou errado; é preciso entender o que melhor atende ao seu problema. Existem diversos outros métodos, cada um com suas vantagens e desvantagens, como o RAPS, cobertura assímetrica e classe condicional, conhecida também como [Mondrian](https://alrw.net/old/04.pdf). 

![image 2.png|A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification](/img/user/digital-garden/meu-modelo-conforme/assets/image%202.png)
[A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification](https://arxiv.org/abs/2107.07511)

![image 3.png|Classe Condicional](/img/user/digital-garden/meu-modelo-conforme/assets/image%203.png)
[Classe Condicional](https://arxiv.org/abs/2107.07511)

A ideia da previsão é ter uma cobertura que tenha a mesma distribuição de dados passados, como dados futuros. Esse conceito é a permutabilidade. Isso significa que, não importa de que época o dado existe, ele irá ter a mesma distribuição de probabilidade. É importante entender isso, por duas razões:

1. Caso haja mudança brusca e mudança na distribuição dos seus dados, a cobertura deverá ser gerada novamente, pois ela deixa de atender para o que foi proposto. Não há necessidade de retreino de modelo, mas sim para a cobertura.
2. Por conta dessa premissa, também não é aconselhado, tampouco incentivado a fazer reamostragem de dados para não afetar a distribuição amostral dos dados. Para condições de dados desbalanceados, irei descrever como resolver no tópico[ Classificador Conforme](/134a0de3378e80c6825bc0e8c4f226e4), preservando a calibração do modelo.

Um ponto que preciso dar ênfase, é que o uso de cobertura não gera apenas um único score como resultado, mas um conjunto de predições de todas as classes, podendo até todas as classes estarem na predição. Isso não é um bug! Modelos conformes que utilizam cobertura geram conjunto de respostas, sendo cada classe a ser testada dentro da cobertura. 

![image 4.png|Understanding, Generating, and Evaluating Prediction Intervals](/img/user/digital-garden/meu-modelo-conforme/assets/image%204.png)
[Understanding, Generating, and Evaluating Prediction Intervals](https://github.com/brshallo/posit-2024)
## Ponto Estimado vs Intervalo

Na previsão conforme, ainda é possível modelar um ponto estimado de duas maneiras:

### 1. Venn-Abers (VA)

O VA é um modelo de calibração de previsão conforme, ao qual oferece:

### 2. P-valor dos NCScores

Podemos estimar o p-valor dos scores de não-conformidade para cada classe predita.

### Considerações Importantes

Em ambos os cenários:

### Interpretação

- Confiança: Indica a probabilidade da classificação predita versus outras classes
- Credibilidade: Mostra o quão adequada é a previsão conforme para classificar a instância

Não abordarei com profundidade estes dois métodos, mas tenha em mente a flexibilidade do framework.

> [!tip] 
> É importante não confundir intervalos de predição com intervalos de confiança, pois são conceitualmente diferentes:
> 
> O intervalo de confiança nos permite estimar, com determinado grau de certeza, onde um parâmetro populacional se encontra.
> 
> Já o intervalo de predição nos permite estimar, também com um grau de certeza específico, onde o valor real de uma nova observação deve estar.

### Cobertura

Utilizar uma cobertura garantida de score de não conformidade pode nos ajudar a interpretar sobre diversos pontos:

![image 5.png|Uncertainty Sets for Image Classifiers using Conformal Prediction](/img/user/digital-garden/meu-modelo-conforme/assets/image%205.png)
[Uncertainty Sets for Image Classifiers using Conformal Prediction](https://people.eecs.berkeley.edu/~angelopoulos/blog/posts/conformal-classification/)

3. Em um modelo de detecção de imagens, conseguimos saber quais labels estão sendo apresentadas para respectivas imagens, e quais têm quantificações aproximadas de incerteza.
4. Em modelos de automação ou detecção, podemos solicitar analise humana aonde mais de uma label é detectada.
5. Em um modelo de recomendação, é possível identificar similaridades entre produtos, sendo possível juntar ambos ou até ofertarem juntos.

```python
y_prob = rf.predict_proba(X_test)
ncscore = 1 - y_prob
(ncscore <= qhat).astype(int)
```

As possíveis respostas, poderão ser, pensando em um classificador binário:

- [0, 0]: Há garantia na probabilidade de que o dado escorado não compõe nenhuma classe.
- [1, 0]: Há garantia de que o dado compõe a classe ‘0’. 
- [0, 1]: Há garantia de que o dado compõe a classe ‘1’. 
- [1, 1]: Há garantia de que o dado compõem ambas as classes.

O código para fazermos futuras escoragens é bem simples, a complexidade se dá aos fundamentos de chegarmos em um bom entendimento.

Se ainda não ficou muito claro sobre o tema, antes de falarmos sobre validação e eficiência, sugiro seguir os exemplos de Christoph Molnar:

[Week #1: Getting Started With Conformal Prediction For Classification](https://mindfulmodeler.substack.com/p/week-1-getting-started-with-conformal)
