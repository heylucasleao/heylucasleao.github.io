---
{"dg-publish":true,"permalink":"/digital-garden/meu-modelo-conforme/text/5-treinamento/"}
---

# Treinamento

Nos meus projetos iniciais de carreira, como o de [ansiedade e pandemia em √©poca de Covid-19](https://heylucasleao-ansiedade-e-pandemia-streamlit-app-5889dq.streamlit.app/) ou [o balan√ßo sobre os dados e previsibilidade sobre a pandemia na regi√£o do Amazonas](https://heylucasleao-covid-no-amazonas-streamlit-app-bxa3n1.streamlit.app/), eu acreditava que apenas gerar o F1 Score ou analisar o ROC-AUC resolveria todos os meus problemas. Por√©m, quando me deparava com novos dados, eu n√£o sabia o que estava acontecendo. Percebi, tarde demais, que o modelo n√£o estava calibrado! 

![image 1.png|Introduction To Conformal Prediction](/img/user/digital-garden/meu-modelo-conforme/assets/image%201.png)

Para resolver esse problema, uma op√ß√£o √© transformar nosso modelo em um modelo conforme, que descrevi anteriormente. Eu at√© poderia ter parado apenas na camada de calibra√ß√£o Venn-Abers, que por si s√≥ j√° transforma nosso modelo em um conforme, mas a gera√ß√£o de cobertura nos d√° outras interpretabilidades e possibilidades, que irei comentar no pr√≥ximos t√≥picos.

> [!tip] üí°
> N√£o h√° necessidade de termos uma modelo de calibra√ß√£o se quiser calibrar fazendo uma cobertura de previs√£o conforme. Apenas fa√ßo para **melhorar** integridade das probabilidades.

## Divis√£o de Dados

Vamos seguir este processo:

- Durante o treinamento, dividiremos os dados em tr√™s partes: dados de treinamento, **calibra√ß√£o** e valida√ß√£o.

Por motivos de vi√©s, n√£o utilizamos a mesma massa de dados de treinamento do modelo para gerar o nosso ponto de corte para cobertura de dados conformes. 

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_calib, y_train, y_calib = train_test_split(X_train, y_train, test_size=0.25, random_state=42)
```

- Por conta de meu projeto, vou permanecer com RandomForestClassifier, mas pode ser qualquer um. A fim de simplicidade, deixarei tamb√©m para termos apenas 2 classes de previs√£o.

```python
rf = RandomForestClassifier(random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
```

## Grau de Incerteza

Precisamos tornar esse modelo conforme. Para isso, vamos estabelecer um limite m√°ximo de incerteza do modelo para determinar se a classe predita √© provavelmente a verdadeira.

Esse grau de incerteza √© chamado de score de n√£o-conformidade. Existem v√°rias maneiras de calcul√°-lo, mas para simplificar, usaremos o Hinge Loss, tamb√©m conhecido como probabilidade inversa. Este score √© calculado como 1 ‚àí P(y), onde P(y) √© a predi√ß√£o do seu modelo. Podemos interpret√°-lo como a dist√¢ncia entre a previs√£o do nosso modelo e a label verdadeira. Quanto mais pr√≥ximo de 1, mais incerto √© o resultado; quanto mais pr√≥ximo de 0, mais certo.

## Cobertura

Durante o treinamento utilizamos esse score **apenas** para as probabilidades das classes verdadeiras, a fim de gerar um ponto de corte. Esse ponto separa novos dados que podem conter cada uma das classes em potencial. Chamamos isso de **cobertura**, pois garante com um grau de confiabilidade que a label verdadeira estar√° dentro do conjunto de predi√ß√£o. Abordaremos esse conceito com mais detalhes posteriormente.

```python
# Total de Dados
n = len(X_calib)

# Score do Modelo
y_prob = model.predict_proba(X_calib)

# Filtramos apenas para as probabilidades das labels verdadeiras
y_prob = y_prob[np.arange(n),y_calib]

# Hinge Loss
non_conformity_score = 1 - y_prob
```

Uma vez gerado, criaremos o qÃÇ , que √© um ponto de corte baseado em medida de posi√ß√£o. Para todos os scores de n√£o-conformidade, ordenamos seus valores do maior para o menor. Isso nos permite tra√ßar um quantil baseado no n√≠vel de confian√ßa, garantindo um percentual de cobertura.

Recapitulando, o processo envolve:

1. Definir nosso n√≠vel de confian√ßa Œ±.
2. Gerar o qÃÇ, que ser√° o quantil (1 - Œ±) desse score ordenado, do maior para o menor. Esse √© o percentual de garantia. Por exemplo: com um Œ± de 0.10, teremos um ponto de corte com 90% de garantia.

```python
n = len(nonconformity_score)
alpha = 0.1
q_level = np.ceil((n + 1) * (1 - alpha)) / n
qhat = np.quantile(nonconformity_score, q_level, method="higher")
```

> [!tip] üí°
> 
> > O q_level √© necess√°rio para ajustar a posi√ß√£o do quantil dentro do conjunto de dados finito, distribuindo adequadamente as posi√ß√µes dos quantis.

Com todos os elementos gerados, podemos plotar o resultado. √â importante lembrar que tudo abaixo desse qÃÇ tem **cobertura garantida**. Dessa forma, os 90% de cobertura significam que h√° a garantia de 90% para a classe verdadeira estar contida no conjunto de predi√ß√£o. √â por essa raz√£o que se utiliza o termo "incerteza rigorosa" para Previs√£o Conforme ‚Äî existe uma probabilidade emp√≠rica por tr√°s disso!

```python
fig = px.histogram(
    nonconformity_score, 
    title="Score de N√£o Conformidade", 
    color_discrete_sequence=["grey", "orange"],
    width=800,
    height=400)
fig.update_yaxes(title_text="Total")
fig.update_xaxes(title_text="Score")
fig.update_layout(legend=dict(title="Label"))
fig.add_vline(x=qhat, line_dash="dash", line_color="black", annotation_text="qhat", annotation_position="top")
```

![newplot.png](/img/user/digital-garden/meu-modelo-conforme/assets/newplot.png)