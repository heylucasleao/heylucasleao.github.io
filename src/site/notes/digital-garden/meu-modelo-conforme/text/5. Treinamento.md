---
{"dg-publish":true,"permalink":"/digital-garden/meu-modelo-conforme/text/5-treinamento/"}
---

# Treinamento

Nos meus projetos iniciais de carreira, como o de [ansiedade e pandemia em época de Covid-19](https://heylucasleao-ansiedade-e-pandemia-streamlit-app-5889dq.streamlit.app/) ou [o balanço sobre os dados e previsibilidade sobre a pandemia na região do Amazonas](https://heylucasleao-covid-no-amazonas-streamlit-app-bxa3n1.streamlit.app/), eu acreditava que apenas gerar o F1 Score ou analisar o ROC-AUC resolveria todos os meus problemas. Porém, quando me deparava com novos dados, eu não sabia o que estava acontecendo. Percebi, tarde demais, que o modelo não estava calibrado! 

![image 1.png|Introduction To Conformal Prediction](/img/user/digital-garden/meu-modelo-conforme/assets/image%201.png)
[Introduction To Conformal Prediction](https://leanpub.com/conformal-prediction)

Para resolver esse problema, uma opção é transformar nosso modelo em um modelo conforme, que descrevi anteriormente. Eu até poderia ter parado apenas na camada de calibração Venn-Abers, que por si só já transforma nosso modelo em um conforme, mas a geração de cobertura nos dá outras interpretabilidades e possibilidades, que irei comentar no próximos tópicos.

> [!tip] 
> Não há necessidade de termos uma modelo de calibração se quiser calibrar fazendo uma cobertura de previsão conforme. Apenas faço para **melhorar** integridade das probabilidades.

## Divisão de Dados

Vamos seguir este processo:

- Durante o treinamento, dividiremos os dados em três partes: dados de treinamento, **calibração** e validação.

Por motivos de viés, não utilizamos a mesma massa de dados de treinamento do modelo para gerar o nosso ponto de corte para cobertura de dados conformes. 

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_calib, y_train, y_calib = train_test_split(X_train, y_train, test_size=0.25, random_state=42)
```

- Por conta de meu projeto, vou permanecer com RandomForestClassifier, mas pode ser qualquer um. A fim de simplicidade, deixarei também para termos apenas 2 classes de previsão.

```python
rf = RandomForestClassifier(random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
```

## Grau de Incerteza

Precisamos tornar esse modelo conforme. Para isso, vamos estabelecer um limite máximo de incerteza do modelo para determinar se a classe predita é provavelmente a verdadeira.

Esse grau de incerteza é chamado de score de não-conformidade. Existem várias maneiras de calculá-lo, mas para simplificar, usaremos o Hinge Loss, também conhecido como probabilidade inversa. Este score é calculado como 1 − P(y), onde P(y) é a predição do seu modelo. Podemos interpretá-lo como a distância entre a previsão do nosso modelo e a label verdadeira. Quanto mais próximo de 1, mais incerto é o resultado; quanto mais próximo de 0, mais certo.

## Cobertura

Durante o treinamento utilizamos esse score **apenas** para as probabilidades das classes verdadeiras, a fim de gerar um ponto de corte. Esse ponto separa novos dados que podem conter cada uma das classes em potencial. Chamamos isso de **cobertura**, pois garante com um grau de confiabilidade que a label verdadeira estará dentro do conjunto de predição. Abordaremos esse conceito com mais detalhes posteriormente.

```python
# Total de Dados
n = len(X_calib)

# Score do Modelo
y_prob = model.predict_proba(X_calib)

# Filtramos apenas para as probabilidades das labels verdadeiras
y_prob = y_prob[np.arange(n),y_calib]

# Hinge Loss
non_conformity_score = 1 - y_prob
```

Uma vez gerado, criaremos o q̂ , que é um ponto de corte baseado em medida de posição. Para todos os scores de não-conformidade, ordenamos seus valores do maior para o menor. Isso nos permite traçar um quantil baseado no nível de confiança, garantindo um percentual de cobertura.

Recapitulando, o processo envolve:

1. Definir nosso nível de confiança α.
2. Gerar o q̂, que será o quantil (1 - α) desse score ordenado, do maior para o menor. Esse é o percentual de garantia. Por exemplo: com um α de 0.10, teremos um ponto de corte com 90% de garantia.

```python
n = len(nonconformity_score)
alpha = 0.1
q_level = np.ceil((n + 1) * (1 - alpha)) / n
qhat = np.quantile(nonconformity_score, q_level, method="higher")
```

> [!tip] 
> 
>  O q_level é necessário para ajustar a posição do quantil dentro do conjunto de dados finito, distribuindo adequadamente as posições dos quantis.

Com todos os elementos gerados, podemos plotar o resultado. É importante lembrar que tudo abaixo desse q̂ tem **cobertura garantida**. Dessa forma, os 90% de cobertura significam que há a garantia de 90% para a classe verdadeira estar contida no conjunto de predição. É por essa razão que se utiliza o termo "incerteza rigorosa" para Previsão Conforme — existe uma probabilidade empírica por trás disso!

```python
fig = px.histogram(
    nonconformity_score, 
    title="Score de Não Conformidade", 
    color_discrete_sequence=["grey", "orange"],
    width=800,
    height=400)
fig.update_yaxes(title_text="Total")
fig.update_xaxes(title_text="Score")
fig.update_layout(legend=dict(title="Label"))
fig.add_vline(x=qhat, line_dash="dash", line_color="black", annotation_text="qhat", annotation_position="top")
```

![newplot.png](/img/user/digital-garden/meu-modelo-conforme/assets/newplot.png)