---
{"dg-publish":true,"permalink":"/digital-garden/meu-modelo-conforme/text/10-classificador-conforme/"}
---

# Classificador Conforme

Parabéns por chegar a este tópico. Agora, vamos explorar a síntese de todos os conceitos discutidos anteriormente e apresentar uma visão detalhada do modelo que criei. Certamente, pode haver conflitos conceituais. Caso ocorra, não tenha medo de me comunicar. Objetivo desse diário é demonstrar minha linha de pensamento, e como cheguei até aqui.

O objetivo era desenvolver um modelo com alta adaptabilidade, flexibilidade e características específicas oferecidas pelo RandomForest, incluindo:

- Capacidade de processar dados de alta dimensionalidade e extrair correlações complexas.
- Habilidade de treinar modelos com dados de baixa frequência.
- Robustez no tratamento de conjuntos de dados desbalanceados.
- Eficiência computacional, permitindo treinamento e inferência rápidas.

Ao incorporar a calibração conformal (Venn-Abers), o modelo alcança:

- Regulação precisa do score, transformando-o em uma medida verdadeiramente probabilística e mitigando riscos de overfitting.
- Maior confiabilidade nas previsões, essencial para aplicações críticas.

A implementação da Previsão Conforme proporciona:

- Aprimoramento significativo na interpretabilidade dos resultados do modelo.
- Estabelecimento de uma garantia de cobertura, permitindo controle preciso sobre a confiança do modelo.
- Flexibilidade na modelagem do intervalo de predição, adaptável a diferentes cenários.
- Capacidade de quantificar a incerteza das previsões.

A integração de propriedades de Aprendizado Sensível ao Custo oferece:

- Versatilidade para lidar com diversos tipos de distribuições de dados.
- Calibração baseada no custo de erro e não somente performance.
- Capacidade de ajustar o modelo para minimizar custos em cenários assimétricos de erro.

![image 7.png](/img/user/digital-garden/meu-modelo-conforme/assets/image%207.png)

## [Classificador Conforme Binário](https://github.com/HeyLucasLeao/cp-study/blob/master/models/hinge/cp.py)

Um diferencial que não mencionei é a utilização de uma ideia baseada nos artigos [Venn Prediction for Survival Analysis](https://oa.upm.es/64515/1/TFM_IGNACIO_APARICIO_VAZQUEZ.pdf) e [Efficient Venn predictors using random forests](https://link.springer.com/article/10.1007/s10994-018-5753-x). Nessa abordagem, os dados não utilizados no treinamento de cada árvore, conhecidos como amostras OOB (Out-of-Bag), são empregados para gerar nossa cobertura. A ideia é que teremos uma representatividade populacional de cada arvore de decisão. Em teoria, isso elimina a necessidade de uma base de calibração separada. No entanto, neste caso, utilizo esses dados para outro propósito.

É importante ressaltar também que transformo o conjunto de predições em uma única resposta: se o modelo tem certeza de que há apenas o rótulo verdadeiro ou não. Isso compromete a interpretabilidade do conjunto de predições do modelo. Conceitualmente, reconheço que isso pode ser questionável, mas o faço para obter flexibilidade na utilização do Aprendizado Sensível a Custo. Vale notar que internamente ainda é possível recuperar essa informação.

### Calibração do Valor α

Após a criação do modelo conforme, ainda não há uma definição clara do valor α. Para determiná-lo, calibro para o melhor resultado da acurácia balanceada em um intervalo de 0.01 a 0.10. O objetivo é encontrar a melhor cobertura que permita ao modelo obter o menor custo de erro com uma garantia mínima de 90%. Haverá situações em que não há razão para reduzir α, possibilitando seu estreitamento, enquanto em outras, o modelo poderá ter uma eficiência melhor com um aumento desse valor.

> [!tip] 
> Ao unir os três tópicos e calibrar o α, estabelecemos um nível de significância que equilibra o custo entre falsos positivos e falsos negativos, evita overfitting e mantém uma distribuição representativa para a cobertura.

### Avaliação de Performance

Para certificar que tudo ocorreu bem com o modelo, verifico seu performance com dados de validação utilizando uma função chamada `evaluate`. As métricas que ele retorna são:

```javascript
- "total": A quantidade total de dados utilizados para avaliação.
- "alpha": O nível de significância estabelecido da cobertura.
- "empirical_coverage": A cobertura empírica dos conjuntos da Previsão Conforme.
- "one_c": A proporção de conjuntos de predição contendo exatamente um elemento.
- "avg_c": O tamanho médio dos conjuntos de predição.
- "empty": A proporção de conjuntos de predição vazios.
- "error": A taxa de erro de classificação.
- "log_loss": A perda logarítmica das predições.
- "ece": O erro de calibração esperado.
- "bm": O índice de informação do bookmaker.
- "mcc": O coeficiente de correlação de Matthews.
- "f1": O F1-Score
- "fpr": A taxa de falsos positivos.
```

### Distribuição Beta

![1000000056.png|Beta Distribution: Uses, Parameters & Examples](/img/user/digital-garden/meu-modelo-conforme/assets/1000000056.png)

A distribuição Beta é uma distribuição de probabilidade contínua no intervalo [0, 1], ideal para observar e quantificar a incerteza na probabilidade do modelo. Sua função de densidade de probabilidade é definida por dois parâmetros α e β, que controlam a forma da distribuição e permitem modelar diferentes tipos de incerteza probabilística. É importante destacar que, devido ao rigor da Previsão Conforme, a cobertura segue naturalmente essa distribuição, assegurando a confiabilidade do modelo. Embora não seja estritamente necessário analisar a Beta especificamente para este fim, não discrimino o uso para análises posteriores, como nos testes A/B.

## Referência de meu Projeto

[https://github.com/HeyLucasLeao/cp-study/blob/master/cp.ipynb](https://github.com/HeyLucasLeao/cp-study/blob/master/cp.ipynb)
