---
{"dg-publish":true,"permalink":"/digital-garden/meu-modelo-conforme/text/3-probabilidade-e-calibracao-de-modelo/"}
---

# Probabilidade e Calibração de Modelo

A probabilidade do modelo é um conceito crucial na aprendizagem de máquina. Ela representa a confiança do modelo em suas previsões, oferecendo insights sobre seu desempenho e confiabilidade.

A ideia em geral é que essa probabilidade, seja empírica, ou seja, refletindo que dados similares caem nesse percentual. 

Imagine um modelo de visão computacional que verifica se as cerejas do café estão maduras o suficiente para colheita. Se separarmos todas as cerejas às quais o modelo atribuiu um score probabilístico de 0.80 e, manualmente, verificarmos que 40% delas estão realmente maduras, logo que o modelo está descalibrado. Isso poderá trazer risco a colheita, dando falsas expectativas e sendo imprevisivel seus resultados, demonstrando uma alta confiança. Em contra partida, a calibração quando bem feita oferece maior segurança de que o modelo se comportará adequadamente em um ambiente de produção, além de fornecer informações importantes sobre as incertezas do modelo, proporcionando previsibilidade para os stakeholders.

Isto é bem exemplificado no artigo [Calibration: the Achilles heel of predictiveanalytics](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-019-1466-7):

> Se o algoritmo é usado para informar pacientes, estimativas de risco mal calibradas levam a falsas expectativas tanto para pacientes quanto para profissionais de saúde. Os pacientes podem tomar decisões pessoais antecipando um evento, ou sua ausência, que na verdade eram equivocadas. Por exemplo, considere um modelo de predição que prevê a chance de um tratamento de fertilização in vitro (FIV) resultar em um nascimento [[14](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-019-1466-7#ref-CR14)]. Independentemente do quão bem os modelos possam discriminar entre tratamentos que resultam em nascimento versus aqueles que não resultam, é claro que uma super ou subestimação da chance de nascimento torna os algoritmos clinicamente inaceitáveis. Por exemplo, uma forte superestimação da chance de nascimento após FIV daria falsas esperanças a casais que já estão passando por uma experiência estressante e emocional. Tratar um casal que, na realidade, tem um prognóstico favorável expõe a mulher desnecessariamente a possíveis efeitos colaterais prejudiciais, como a síndrome de hiperestimulação ovariana.

[Probability Calibration : Data Science Concepts](https://www.youtube.com/watch?v=AunotauS5yI)

## Não é um problema novo

Glenn W. Brier, meteorologista, enfrentou um problema similar em meados do século 20 com previsões do tempo. Como poderia avaliar a precisão de suas estimativas em relação ao que realmente acontecia? Foi assim que desenvolveu o "Brier Score", uma métrica que avalia a precisão das previsões probabilísticas, comparando as probabilidades previstas com os resultados observados. Um score menor indica previsões mais precisas. Até os dias de hoje, é uma métrica essencial para entendermos o comportamento de nossas previsões.

[The Brier Score Explained | Model Calibration](https://www.youtube.com/watch?v=BiaebXlgfNQ)

Outra métrica utilizada nesse contexto é o Log Loss, que quantifica a incerteza na probabilidade das predições do modelo, comparando-as com os resultados reais. Utilizar essas duas métricas pode fornecer perspectivas diferentes sobre o desempenho do modelo.

Em geral, modelos de ML **não** garantem boa calibração, ou seja, seu score probabilístico pode não representar realmente uma probabilidade. Isso é evidenciado na [palestra](https://youtu.be/6YnhoCfArQo) de [Guillaume Lemaitre](https://www.linkedin.com/in/guillaume-lemaitre-b9404939/), que também demonstra como técnicas populares de balanceamento podem prejudicar bastante essa calibração.

Uma forma de observar isso é através da curva de confiabilidade, que compara as probabilidades previstas pelo modelo com as frequências reais observadas dos eventos. Ela demonstra se o modelo está bem calibrado, confiante demais ou não. Para comparação, é criada uma linha diagonal perfeita que representa um modelo perfeitamente calibrado, onde as probabilidades previstas correspondem exatamente às frequências observadas. Se a curva de predição ficar abaixo da linha diagonal, significa que o modelo está confiante demais, prevendo probabilidades mais altas do que deveria em relação ao percentual real de ocorrências. Caso a curva fique acima da linha diagonal, sugere que o modelo está subestimando suas previsões, sendo menos confiante do que deveria.

![image.png|BMC Medical Informatics and Decision Making](/img/user/digital-garden/meu-modelo-conforme/assets/image.png)
[BMC Medical Informatics and Decision Making](https://www.researchgate.net/figure/Reliability-curve-calibration-plot-of-the-model-output-probabilities-on-the-test-set_fig3_329597860)

Tendo isso em mente, vamos considerar um cenário: você está desenvolvendo um modelo de predição para identificar quando seu pedido no iFood pode resultar em uma experiência ruim. Para isso, você coletou 10 características de 100.000 pedidos (sim, você adora pedir comida!) ao longo de 5 anos. Ao analisar os dados, você percebeu que a distribuição é desbalanceada — apenas 20% dos registros indicam uma experiência negativa. Isso é um bom sinal, mas como seu modelo se comportaria com esse desbalanceamento? Você decide, então, experimentar quatro tipos de modelos:

1. [RandomForestClassifier (RF) sem nenhum tratamento nos dados](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
2. [BalancedRandomForestClassifier](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.BalancedRandomForestClassifier.html)
3. RF utilizando a técnica de [Random Under-Sampling (RUS)](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html)
4. RF utilizando a técnica de Synthetic Minority [Over-sampling Technique (SMOTE)](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)
[https://github.com/HeyLucasLeao/cp-study/blob/master/calibration_demo.ipynb](https://github.com/HeyLucasLeao/cp-study/blob/master/calibration_demo.ipynb)

Feito esses quatro modelos, se depara com isso:

## Curva de Confiabilidade

![vanilla_rf.png|RandomForestClassifier](/img/user/digital-garden/meu-modelo-conforme/assets/vanilla_rf.png)
RandomForestClassifier


![balanced_rf.png|BalancedRandomForestClassifier](/img/user/digital-garden/meu-modelo-conforme/assets/balanced_rf.png)
BalancedRandomForestClassifier

![rus_rf.png|RUS RandomForestClassifier](/img/user/digital-garden/meu-modelo-conforme/assets/rus_rf.png)
RUS RandomForestClassifier

![smote_rf.png|SMOTE RandomForestClassifier](/img/user/digital-garden/meu-modelo-conforme/assets/smote_rf.png)
SMOTE RandomForestClassifier
O modelo sem tratamento do desbalanceamento de dados é, na verdade, melhor calibrado do que aqueles que utilizaram técnicas de reamostragem, como SMOTE, ROS ou RUS. Isto também é evidenciado em diversos, artigo, como por exemplo:

- [The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression](https://doi.org/10.1093/jamia/ocac093)
- [The harms of class imbalance corrections for machine learning based prediction models: a simulation study](https://arxiv.org/abs/2404.19494)
- [Stop Oversampling for Class Imbalance Learning: A Critical Review](https://arxiv.org/abs/2202.03579)

Além disso, observa-se que alguns modelos são naturalmente descalibrados após seu treinamento, independentemente do tratamento de dados. Conforme demonstrado no artigo [Probabilistic Prediction in scikit-learn](https://www.diva-portal.org/smash/record.jsf?dswid=5765&pid=diva2%3A1603345&c=1&searchType=SIMPLE&language=en&query=2%3A1603345&af=%5B%5D&aq=%5B%5B%5D%5D&aq2=%5B%5B%5D%5D&aqe=%5B%5D&noOfRows=50&sortOrder=author_sort_asc&sortOrder2=title_sort_asc&onlyFullText=false&sf=all), é necessário adicionar um método de calibração para melhorar as estimativas. 

### Venn-Abers

Nesse contexto, utilizo o [Venn-Abers](https://youtu.be/TK37Db3WHYI?t=2344), um modelo de calibração que fornece um intervalo de probabilidade, auxiliando na interpretação e quantificação da incerteza na previsão de um modelo. Para obter as probabilidades, o método usa [regressão isotônica](https://www.youtube.com/watch?v=TEbLUhHqLgE) para calibrar as previsões, aplicando-a duas vezes: primeiro assumindo que o rótulo verdadeiro é 0, depois assumindo que é 1. Esse processo gera duas funções de calibração, f0 e f1, que calculam os intervalos de probabilidade e representam os limites inferior e superior da probabilidade do rótulo ser 1. É possível também trabalhar com uma única probabilidade como resultado, que é o que faremos neste projeto. O artigo original sugere a fórmula:

$$
p = \frac{p_1}{1 - p_0 + p_1}
$$

### Reamostragem

Considerando que a reamostragem prejudica a calibração do modelo, decidi abordar o problema das classes de outra maneira, preservando a distribuição original dos dados. É importante ressaltar também que o desbalanceamento por si só nem sempre prejudica o modelo — há arquiteturas pensadas nestes cenários, regularizando o viés. Durante anos, existiu uma concepção de que árvores de decisão, quando aplicadas em dados desbalanceados, inevitavelmente produziriam modelos enviesados para a classe majoritária. Entretanto, existem evidências de que árvores de decisão são capazes de lidar com esse cenário e, em algumas situações, podem até desenvolver uma tendência favorável à classe minoritária, com ajustes específicos. No caso do RandomForestClassifier, por exemplo, ajustamos o critério do modelo com base na proporção das classes, o que auxilia na generalização. Para um entendimento mais profundo, sugiro a leitura do artigo [Towards understanding the bias in decision trees](https://arxiv.org/abs/2501.04903).
