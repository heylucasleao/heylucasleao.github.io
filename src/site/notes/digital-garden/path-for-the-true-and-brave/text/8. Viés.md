---
{"dg-publish":true,"permalink":"/digital-garden/path-for-the-true-and-brave/text/8-vies/"}
---

# Viés

O viés ocorre porque a variável de confusão cria uma **associação espúria** (falsa) entre o **Tratamento** (a Causa, A) e o **Resultado** (o Efeito, B). Isso acontece quando o Tratamento e o Resultado **compartilham uma causa comum** (a variável de confusão, C).
Em outras palavras, a variável de confusão (C) influencia:

1. A **probabilidade de receber o Tratamento** ($C \rightarrow A$).
2. O **Resultado de interesse** ($C \rightarrow B$).

Graficamente, a relação espúria é representada no **DAG (Modelo Gráfico Acíclico Direcionado)** como um caminho backdoor: $A \leftarrow C \rightarrow B$
Sem **controlar adequadamente** (ajustar ou condicionar) essa variável, a nossa estimativa do efeito de A em B incluirá o efeito indireto de C em B, fazendo parecer que A tem um efeito maior (ou menor) do que realmente tem.
# Viés de seleção

Surge quando a amostra analisada não representa fielmente a população de interesse, devido a um critério de seleção (implícito ou explícito) que correlaciona a probabilidade de receber o Tratamento com o próprio Resultado. Esse fenômeno pode ocorrer tanto na etapa de coleta de dados quanto na divisão dos grupos, comprometendo a validade interna e externa das estimativas. Por essa razão, é fundamental dedicar atenção ao **[[digital-garden/path-for-the-true-and-brave/text/5. Design de Experimentos\|5. Design de Experimentos]]** e ao mecanismo de atribuição, garantindo que o processo de entrada no estudo não introduza correlações artificiais que invalidem a inferência causal.
### Exemplos

1. **Autoseleção:** Quando o indivíduo decide se quer participar ou não do tratamento. Pessoas que se voluntariam para um programa de treinamento profissional podem ser mais motivadas do que as que não se voluntariam. Se a "motivação" afeta o salário final, comparar voluntários com não-voluntários enviesará o efeito do treinamento.
    
2. **Viés de Atrito:** Comum em estudos longitudinais. Se os participantes que abandonam o estudo são sistematicamente diferentes dos que ficam (ex: em um teste de medicamento, os que sentem muitos efeitos colaterais desistem), a análise final será feita apenas sobre os "sobreviventes", distorcendo o efeito real.
    
3. **Truncamento Incidental:** Ocorre quando observamos o resultado $Y$ apenas para uma subamostra selecionada. O exemplo clássico é a "Equação de Salários de Heckman": só observamos o salário de quem decide trabalhar. Se a decisão de trabalhar estiver correlacionada com habilidades não observadas, a média salarial da amostra não representa a média da população.

### Formalização

Para que a estimativa do efeito causal seja válida, precisamos que a atribuição do tratamento seja independente dos resultados potenciais (Independência). O viés de seleção quebra essa premissa:

$$(Y_1, Y_0) \not\perp D \mid S=1$$

Onde $S=1$ indica que o indivíduo faz parte da amostra observada. Isso implica que:

- $E[Y_0 | D=1, S=1] \neq E[Y_0 | D=0, S=1]$
    

Ou seja, mesmo na ausência do tratamento, o grupo que foi "selecionado" para o tratamento teria um desempenho diferente do grupo de controle devido a características inerentes à seleção.
# Viés de variável omitida (OVB) & Endogeneidade

>[!Abstract] Leitura Recomendada
> _[The anatomy of omitted variable bias](https://www.everydaycausal.com/causal-frameworks.html#sec-ovb)_

A **Endogeneidade** ocorre quando uma variável explicativa em um modelo de regressão é correlacionada com o termo de erro ($\epsilon$). 

O **OVB** é uma das formas mais frequentes de endogeneidade. Ele surge quando omitimos do modelo uma variável $X_{omitida}$ que deveria estar presente. Para que essa omissão gere viés no coeficiente da variável de tratamento $D$, duas condições devem ser atendidas simultaneamente: 

- **Relevância para o Outcome:** A variável omitida deve ser um determinante de $Y$.
    
- **Correlação com o Tratamento:** A variável omitida deve estar correlacionada com a variável de tratamento $D$.

### Impacto no Modelo

Quando essas condições são atendidas, o efeito da variável omitida é "capturado" pelo termo de erro, tornando-o correlacionado com o tratamento. Matematicamente, o viés pode ser expressado como:

$$Bias = \beta_{omitida} \cdot \delta_{DX}$$

Onde $\beta_{omitida}$ é o efeito da variável omitida sobre $Y$, e $\delta_{DX}$ resulta da regressão da variável omitida contra o tratamento. Como consequência, o estimador de MQO torna-se viesado e inconsistente, falhando em capturar o verdadeiro efeito causal.

> [!tip] Nota
> Uma análise rápida de verificar uma variável de confusão sob isso é durante a análise exploratória utilizando `pairplot` da biblioteca Seaborn.
> Nela, conseguiremos ver se há uma correlação forte das covariáveis com o tratamento.
> 
> `sns.pairplot(data[["T", "C1", "C2", "C3"]], diag_kind="hist")`
## Derivação do Estimador de Mínimos Quadrados Ordinários (MQO)

Modelo verdadeiro $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + u$, onde $E[u] = 0$ e $\operatorname{Cov}(u, X_1) = \operatorname{Cov}(u, X_2) = 0$

Suponha que estimemos por MQO apenas $Y$ sobre $X_1$ (isto é, omitimos $X_2$). O estimador de MQO para o coeficiente de $X_1$ na regressão simples é  $\alpha_1 = \frac{\operatorname{Cov}(Y, X_1)}{\operatorname{Var}(X_1)}$. 

Substituindo o modelo verdadeiro em $\operatorname{Cov}(Y, X_1)$:  

$$\operatorname{Cov}(Y, X_1) = \operatorname{Cov}(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + u, X_1) = \beta_1 \operatorname{Var}(X_1) + \beta_2 \operatorname{Cov}(X_2, X_1) + \operatorname{Cov}(u, X_1)$$

Assumindo ($\operatorname{Cov}(u, X_1)=0$), temos: $\alpha_1 = \beta_1 + \beta_2 \frac{\operatorname{Cov}(X_2, X_1)}{\operatorname{Var}(X_1)}$

Portanto, o valor esperado do estimador é $E[\alpha_1] = \beta_1 + \beta_2 \frac{\operatorname{Cov}(X_2, X_1)}{\operatorname{Var}(X_1)}.$ 

O termo adicional $\beta_2 \dfrac{\operatorname{Cov}(X_2, X_1)}{\operatorname{Var}(X_1)}$ é o viés por omissão. Ele será diferente de zero sempre que:

- $\beta_2 \neq 0$ (ou seja, $X_2$ afeta $Y$) e
- $\operatorname{Cov}(X_2, X_1) \neq 0$ (ou seja, $X_2$ está correlacionada com $X_1$).

Em outras palavras: se $X_2$ é relevante para $Y$ e está correlacionada com $X_1$, então ao omiti-la o efeito de $X_2$ é parcialmente atribuído a $X_1$, enviesando $\alpha_1$.

>[!tip] Exemplo
>Podemos ver uma representação de OVB e como avaliar variáveis de controle em uma regressão a partir da aula ministrada em _[Mastering Mostly Harmless Econometrics - Part 2](https://www.aeaweb.org/webcasts/2020/mastering-mostly-harmless-econometrics-part-2)_, a partir de 01:14:00.

# A Necessidade de Raciocínio Causal

Voltando menos para uma formalização, é fundamental compreendermos que a construção adequada do pensamento causal é nossa defesa contra os múltiplos vieses descritos neste capítulo. Como demonstrado em [[digital-garden/path-for-the-true-and-brave/text/5. Design de Experimentos\|5. Design de Experimentos]] e [[digital-garden/path-for-the-true-and-brave/text/6. DAG\|6. DAG]], sem uma análise crítica prévia dos mecanismos causais subjacentes aos dados, corremos o risco não apenas de mascarar os verdadeiros efeitos do tratamento, mas de chegarmos a conclusões opostas à realidade.

O capítulo 6 de The Book of Why - não apenas exemplifica isso, como também vale a leitura - expõe como a dependência exclusiva de análises estatísticos pode nos levar a armadilhas causais. Quando analisamos dados sem mapear as relações causais ficamos vulneráveis a:

- Controlar variáveis que deveríamos deixar livres;
- Ignorar variáveis que deveríamos ajustar;
- Condicionar colisores e criando dependências artificiais;

A lição fundamental é que dados não falam por si só - precisamos de frameworks causais para interpretá-los. Sem essa fundamentação prévia, mesmo as técnicas estatísticas mais avançadas podem nos conduzir a conclusões perigosamente equivocadas. O pensamento causal é nossa principal ferramenta para distinguir entre **o que os dados mostram** e **o que realmente significam** para a tomada de decisão. É a diferença entre ser enganado por padrões estatísticos e compreender verdadeiramente como o mundo funciona.

## Paradoxo de Simpson

O **Paradoxo de Simpson** é um dos exemplos de como variáveis de confusão podem nos enganar. Ele acontece quando uma tendência observada em dados agregados desaparece ou até se inverte quando analisamos os mesmos dados estratificados por subgrupos. 

Imagine dois tratamentos médicos. Olhando os dados gerais, o Tratamento B tem 80% de sucesso versus 70% do Tratamento A - parece óbvio que B é superior. Mas quando separamos por **gravidade** dos pacientes, descobrimos que A é melhor tanto para casos leves (90% vs 85%) quanto para casos graves (68% vs 35%). Isto ocorre pois o Tratamento A foi dado principalmente para casos graves, enquanto B foi usado mais em casos leves, um fator que originalmente não foi analisado no mecanismo de atribuição, prejudicando a análise observacional, pois esta variável é uma causa comum que influencia tanto a escolha do tratamento quanto o resultado. Quando não a contabilizamos, chegamos à uma conclusão errada. 

Esse paradoxo ilustra perfeitamente por que correlações brutas podem ser perigosas. Sem identificar e controlar por confundidores, podemos tomar decisões baseadas em evidências que apontam na direção completamente oposta da realidade. É um lembrete de que dados não se interpretam sozinhos - precisamos sempre do raciocínio causal **antes** da análise estatística. 