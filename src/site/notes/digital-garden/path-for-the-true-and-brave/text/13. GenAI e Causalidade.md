---
{"dg-publish":true,"permalink":"/digital-garden/path-for-the-true-and-brave/text/13-gen-ai-e-causalidade/"}
---

# GenAI e Causalidade

É fundamental salientar que os modelos de linguagem de grande escala (LLMs) não capturam, por si só, relações intrínsecas de causalidade. Existem desafios documentados de raciocínio (reasoning) em modelos não determinísticos e, para fins de inferência causal, essas ferramentas não substituem os conceitos e frameworks consolidados da econometria e estatística.

No episódio do podcast [Free Will, LLMs & Intelligence | Judea Pearl Ep 21 | CausalBanditsPodcast.com](https://www.youtube.com/watch?v=yqKJ9pUQ6Q8&list=PLhKKv6iMja4p5FbJIgzTOE67E1M6c8lnB&index=15), Judea Pearl faz uma analogia na qual confiar exclusivamente em um LLM para causalidade é como ler um livro de receitas escrito por alguém que nunca cozinhou, mas que leu todos os livros de culinária existentes. O modelo replica instruções sem compreender a "química" — por exemplo, por que o fermento faz o bolo crescer. Pearl argumenta que os LLMs não aprendem modelos causais diretamente do ambiente, mas sim "copiam" os modelos mentais dos autores dos textos de treinamento. O resultado é o que ele chama de uma "salada de associações" ou "rumores sobre modelos causais", sugerindo que devemos tratar o modelo como uma nova "caixa preta" para experimentação, e não como uma fonte de verdade causal.

Ainda assim, os LLMs possuem valor como ferramentas de suporte. Eles podem auxiliar na teorização científica, na exploração de potenciais variáveis instrumentais (IVs) ou na identificação de confundidores — desde que operados com cautela. O artigo _[Mining Causality: AI-Assisted Search for Instrumental Variables](https://arxiv.org/abs/2409.14202)_, por exemplo, propõe agentes que auxiliam nessa busca através de etapas estruturadas de _prompt_ e validação. Bibliotecas como a [PyWhyLLM](https://github.com/py-why/pywhyllm) e o site [Causal LLM Agent](https://cais-web.vercel.app/) também surgem como aliados para conectar tratamentos a desfechos.

Estas ferramentas não devem ser utilizadas para terceirizar decisões críticas, nem para substituir o conhecimento de domínio. Seu uso exige rigor, documentação e, crucialmente, revisão por especialistas humanos. Como sugerido pela tradição de Fisher, a interpretação final e a atribuição de efeitos causais são responsabilidades inalienáveis do pesquisador. Quanto maior o impacto da decisão, maior deve ser o rigor da análise humana sobre a saída da máquina.

>[!tip] Causal Reasoning
> Caso tenha interesse para linhas de pesquisa voltado a Large Language Models (LLM), sugiro observar o trabalho de [Zhijing Jing](https://www.youtube.com/@Zhijing), sua linha de pesquisa é voltada a este tema. 