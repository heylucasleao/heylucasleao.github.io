---
{"dg-publish":true,"permalink":"/digital-garden/path-for-the-true-and-brave/text/7-regressao-linear/"}
---

# Regressão Linear

>[!tip] 
>Para uma explicação didática e intuitiva sobre o tema, sugiro ler [Linear regression: our go-to tool - Robson Tigre](https://www.everydaycausal.com/data-models-causality.html).

Os modelos de regressão linear são ferramentas fundamentais na inferência causal. Utilizando o método dos Mínimos Quadrados Ordinários (MQO), é possível estimar parâmetros que, sob as suposições corretas de identificação, possuem uma interpretação causal direta.

Considerando uma relação linear simples $Y \sim X + \varepsilon$, estimamos o efeito de $X$ sobre $Y$ através dos coeficientes $\hat{\beta}_0$ e $\hat{\beta}_1$.

### 1. O Intercepto ($\hat{\beta}_0$)

O coeficiente $\hat{\beta}_0$ representa o valor esperado de $Y$ quando $X = 0$ (assumindo que todas as outras covariáveis do modelo também sejam zero).

- **Ponto de Atenção:** Essa interpretação matemática só possui valor prático se $X = 0$ for um cenário plausível ou observado nos dados (ex: idade = 0 pode não fazer sentido em certas análises laborais).
    

> [!NOTE]
> 
> Interpretação em Experimentos
> 
> Em estudos com grupos de Controle e Tratamento (onde $X$ é uma dummy indicando o tratamento), $\hat{\beta}_0$ representa a média do grupo de controle. Consequentemente, $\hat{\beta}_1$ captura a diferença média entre os grupos (efeito do tratamento).
### 2. O Coeficiente de Interesse ($\hat{\beta}_1$)

O coeficiente $\hat{\beta}_1$ isola a variação média esperada em $Y$ dada uma alteração em $X$, mantendo constantes as demais variáveis do modelo (_ceteris paribus_). A interpretação varia conforme a natureza de $X$:
- **Se $X$ for contínua:** $\hat{\beta}_1$ é a variação média em $Y$ associada a um aumento de uma unidade em $X$.
    
- **Se $X$ for binária (Dummy de Tratamento):** $\hat{\beta}_1$ é interpretado como o Efeito Médio do Tratamento (**ATE** ou **ATT**, dependendo da população condicionada).
### 3. Termo de Interação

Muitas vezes, a suposição de que o efeito de $X$ sobre $Y$ é constante para todos é muito forte. Para capturar como o efeito varia dependendo de uma terceira variável $Z$, incluímos um termo de interação (produto) no modelo:

$$Y = \beta_0 + \beta_1 X + \beta_2 Z + \beta_3 (X \cdot Z) + \varepsilon$$

Neste modelo, o efeito de $X$ sobre $Y$ não é mais apenas $\beta_1$. O efeito marginal de $X$ passa a depender do valor de $Z$:

$$\frac{\Delta Y}{\Delta X} = \beta_1 + \beta_3 Z$$

A interpretação dos coeficientes muda:

- **$\beta_1$ (Efeito Principal de $X$):** Representa o efeito de $X$ sobre $Y$ apenas quando $Z = 0$. Não é mais o efeito médio global.
    
- **$\beta_3$ (Coeficiente de Interação):** Representa o quanto o efeito de $X$ muda para cada unidade adicional de $Z$.
    - Se $Z$ for uma variável binária (ex: $Z=1$ para Mulheres, $Z=0$ para Homens) e $X$ for o tratamento, então $\beta_3$ é a **diferença** de efeito do tratamento entre mulheres e homens, por exemplo.
        

> [!TIP]
> 
> Ao incluir uma interação $X \cdot Z$, sempre inclua as variáveis originais $X$ e $Z$ no modelo separadamente. Omitir $\beta_1$ ou $\beta_2$ força o intercepto ou a inclinação a passar pela origem de forma artificial, enviesando a estimativa da interação.
# O Trade-off entre Viés e Variância

Condicionar covariáveis irrelevantes ou em excesso no modelo pode introduzir um risco: o aumento da variância do estimador de $\hat{\beta}_1$.

O aumento na variância ocorre frequentemente devido à multicolinearidade e significa que as estimativas de $\hat{\beta}_1$ serão menos precisas. Isso resulta em intervalos de confiança mais amplo, tornando mais difícil rejeitar a hipótese nula e obter um resultado estatisticamente significativo.
## Teorema Frisch‑Waugh‑Lovell

Outra forma de observamos isso, é entendermos melhor o funcionamento do teorema Teorema Frisch-Waugh-Lovell (FWL).

Ele demonstra que o coeficiente de $X$ em uma regressão múltipla Y ~ X + Z é igual ao coeficiente obtido ao regressar os resíduos de $Y$ sobre $Z$ nos resíduos de $X$ sobre $Z$. Em termos práticos, isso significa que a contribuição de cada regressora pode ser entendida como a associação entre as partes de $Y$ e $X$ que não são explicadas por $Z$. Isto pode ser dividido em três etapas

1. **Desviesamento (*****Debiasing Step*****)**
	- Remover o viés das variáveis de controle $Z$ da sua variável de interesse $X$.
    - **Regredimos** a variável de tratamento $X$ nas covariáveis de controle $Z$, (ex: $X \sim Z$)
    - Coletamos os **resíduos** dessa regressão $\tilde{X}$. Estes resíduos representam a parte de $X$ que é **ortogonal** (não correlacionada) aos controles $Z$.
2. **Remoção de Ruído (*****Denoising Step*****)**
	- Remover o ruído das variáveis de controle $Z$ da sua variável dependente $Y$.
    - Regredimos a variável dependente $Y$ nas mesmas covariáveis de controle $Z$, (ex: $Y \sim Z$)
	- Coletamos os **resíduos** dessa regressão $\tilde{Y}$. Estes resíduos representam a parte de $Y$ que é **independente** dos controles $Z$.
 3. **Regressão Final**
	- A estimativa final do efeito causal $\hat{\beta}_1$ é obtida regredindo o resíduo da variável dependente no resíduo da variável de tratamento: $\tilde{Y} \sim \tilde{X}$.
	- O $\hat{\beta}$ obtido nesta regressão de resíduos é **idêntico** ao $\hat{\beta}_1$ da regressão original $Y \sim X + Z$. 

Uma melhor explicação sobre este tema pode ser encontrada no livro [Causal Inference in Python: Applying Causal Inference in the Tech Industry, de Matheus Facure](https://www.amazon.com/Causal-Inference-Python-Applying-Industry/dp/1098140257).

Para ver os exemplos via código, [é só acessar aqui.](https://github.com/matheusfacure/causal-inference-in-python-code)

> [!tip]
> Vale relembrarmos, que para atuação de dados categoricos - ou discretizações de valores númericos que queremos atuar como - necessitamos transformá-los como variável Dummy. Para cada coluna, é necessário transformá-lo em uma covariável de 0 ou 1. Assim, o modelo de Regressão irá comportar cada categoria como uma linha adversa, se comportando como uma categoria a parte ao outcome que queremos observar. Utilizando Pandas, podemos utilizar o `pd.get_dummies`. Via modelagem statsmodels, basta dentro da formula ao ols, inserirmos como `outcome ~ C(Variable)`.

> [!tip]
>Lembrete: a regressão linear assume que a relação entre as regressoras e o resultado (condicional) é linear. Se a relação verdadeira for não linear, a especificação linear pode provocar viés de especificação. É importante verificar se isso ocorre, especialmente entre a variável de tratamento e o desfecho; caso ocorra, podemos aplicar transformações adequadas, por exemplo: logaritmos, termos polinomiais, interações ou transformações multiplicativas.
# Regressão Linear como Modelo para Potenciais Outcome

Outra possibilidade de utilizar a regressão linear, é atuar como um modelo de imputação de potenciais resultados. Isto quer dizer que conseguimos estimar os efeitos causais, seja ATE ou ATT, preechendo valores contrafactuais.
A lógica reside na capacidade da regressão de modelar as funções de resultado potencial:$E[Y_0|X]$ e $E[Y_1|X]$.

## Efeito de Tratamento Médio (ATE)

> [!tip] 
> Relembrando! O $ATE$ é calculado como a diferença média entre o que toda a população teria se fosse tratada $\hat{E}[Y_1 | X_i]$ e o que toda a população teria se não fosse tratada $\hat{E}[Y_0 | X_i]$
> - $ATE = \frac{1}{N} \sum_{i} (\hat{E}[Y_1 | X_i] - \hat{E}[Y_0 | X_i])$
> 
> Onde $\hat{E}[Y_0 | X_i]$ e $\hat{E}[Y_1 | X_i]$ são modelos de regressão ajustados, respectivamente, nas unidades de controle $T=0$ e nas unidades tratadas $T=1$.

### Cálculo Simplificado com `statsmodels`

Em um modelo de regressão linear que inclui covariáveis $X$, o estimador do $ATE$ é equivalente ao coeficiente da variável de tratamento $D$.

Se o seu modelo é $Y = \beta_0 + \beta_1 D + \beta_2 X + \epsilon$, a estimativa de $\hat{\beta}_1$ é o $ATE$.

Python

```python
formula_ate = 'Y ~ D + X1 + X2'
model_ate = smf.ols(formula_ate, data=data).fit()
ate_estimate = model_ate.params['D']
```

## Efeito Médio de Tratamento nos Tratados (ATT)

> [!tip]
> Relembrando! O $ATT$ é a diferença média entre o resultado **observado** para o grupo tratado $Y_i$ e o seu resultado contrafactual imputado $\hat{E}[Y_0 | X_i]$
> $ATT = \frac{1}{N_1} \sum_{i: D_i=1} (Y_i - \hat{E}[Y_0 | X_i])$
> Isto significa que usamos o grupo de controle $D=0$ para construir o modelo que prevê o resultado potencial $Y_0$.

### Cálculo Simplificado com `statsmodels`

```python
model_mu0 = smf.ols('Y ~ X1 + X2', data=data[data['D'] == 0]).fit()
imputed_y0 = model_mu0.predict(data[data['D'] == 1]) # Imputar o contrafactual, isto é, usar model_mu0 para prever o Y_0 para as unidades do grupo tratado T=1. 
att_estimate = (data[data['D'] == 1]['Y'] - imputed_y0).mean()
```


