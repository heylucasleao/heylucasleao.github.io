---
{"dg-publish":true,"permalink":"/digital-garden/path-for-the-true-and-brave/text/7-regressao-linear/"}
---

# Regressão Linear

Modelos de regressão linear têm ampla aplicabilidade na inferência causal. Pelo método dos mínimos quadrados ordinários (MQO), conseguimos estimar parâmetros que, sob suposições apropriadas, têm interpretação causal. 

Dada uma relação simples Y ~ X + ε, podemos estimar o efeito de X sobre Y pelas estimativas MQO, $\hat{β}_0$ e $\hat{β}_1$.

> [!tip]
> O coeficiente $\hat{β}_0$ pode ser interpretado como o valor esperado de $Y$ quando $X = 0$ (e quando todas as outras variáveis no modelo são zero). 
> - Atenção: essa interpretação só é útil quando X = 0 é um valor plausível/observado.

> [!tip]
> Em um estudo com grupo de controle e tratamento (X indica tratamento), $\hat{β}_0$ é a média do grupo controle e $\hat{β}_1$ é a diferença média (efeito médio) entre tratamento e controle — desde que as suposições de identificação sejam satisfeitas.

O coeficiente $\hat{β}_1$ representa a variação média esperada em $Y$ resultante de um aumento unitário em $X$, mantendo constantes as demais covariáveis do modelo.
- Se $X$ for a variável de tratamento e for contínua, $\hat{β}_1$ é a variação média de $Y$ associada a um aumento unitário em $X$ (sob suposições causais).
- Se $X$ for uma dummy de tratamento e as suposições de identificação forem plausíveis, $\hat{β}_1$ pode ser interpretado como o Efeito Médio do Tratamento (ATE ou ATT dependendo da população condicionada). Se $Y$ for binário e você usar um modelo linear de probabilidade, $\hat{β}_1$ aproxima o efeito médio (ATE) sob ignorabilidade, mas tem limitações (valores previstos fora de [0,1], heterocedasticidade). Para resultados binários, usar regressão logística.
# O Trade-off entre Viés e Variância

Entretanto, condicionar (ou incluir) covariáveis irrelevantes ou em excesso no modelo pode introduzir um risco: o aumento da variância do estimador de $\hat{\beta}_1$.
O aumento na variância ocorre frequentemente devido à multicolinearidade e significa que as estimativas de $\hat{\beta}_1$ serão menos precisas. Isso resulta em intervalos de confiança mais amplo, tornando mais difícil rejeitar a hipótese nula e obter um resultado estatisticamente significativo.
## Teorema Frisch‑Waugh‑Lovell

Outra forma de observamos isso, é entendermos melhor o funcionamento do teorema Teorema Frisch-Waugh-Lovell (FWL).

Ele demonstra que o coeficiente de $X$ em uma regressão múltipla Y ~ X + Z é igual ao coeficiente obtido ao regressar os resíduos de $Y$ sobre $Z$ nos resíduos de $X$ sobre $Z$. Em termos práticos, isso significa que a contribuição de cada regressora pode ser entendida como a associação entre as partes de $Y$ e $X$ que não são explicadas por $Z$. Isto pode ser dividido em três etapas

1. **Desviesamento (*****Debiasing Step*****)**
	- Remover o viés das variáveis de controle $Z$ da sua variável de interesse $X$.
    - **Regredimos** a variável de tratamento $X$ nas covariáveis de controle $Z$, (ex: $X \sim Z$)
    - Coletamos os **resíduos** dessa regressão $\tilde{X}$. Estes resíduos representam a parte de $X$ que é **ortogonal** (não correlacionada) aos controles $Z$.
2. **Remoção de Ruído (*****Denoising Step*****)**
	- Remover o ruído das variáveis de controle $Z$ da sua variável dependente $Y$.
    - Regredimos a variável dependente $Y$ nas mesmas covariáveis de controle $Z$, (ex: $Y \sim Z$)
	- Coletamos os **resíduos** dessa regressão $\tilde{Y}$. Estes resíduos representam a parte de $Y$ que é **independente** dos controles $Z$.
 3. **Regressão Final**
	- A estimativa final do efeito causal $\hat{\beta}_1$ é obtida regredindo o resíduo da variável dependente no resíduo da variável de tratamento: $\tilde{Y} \sim \tilde{X}$.
	- O $\hat{\beta}$ obtido nesta regressão de resíduos é **idêntico** ao $\hat{\beta}_1$ da regressão original $Y \sim X + Z$. 

Uma melhor explicação sobre este tema pode ser encontrada no livro [Causal Inference in Python: Applying Causal Inference in the Tech Industry, de Matheus Facure](https://www.amazon.com/Causal-Inference-Python-Applying-Industry/dp/1098140257).

Para ver os exemplos via código, [é só acessar aqui.](https://github.com/matheusfacure/causal-inference-in-python-code)

> [!tip]
> Vale relembrarmos, que para atuação de dados categoricos - ou discretizações de valores númericos que queremos atuar como - necessitamos transformá-los como variável Dummy. Para cada coluna, é necessário transformá-lo em uma covariável de 0 ou 1. Assim, o modelo de Regressão irá comportar cada categoria como uma linha adversa, se comportando como uma categoria a parte ao outcome que queremos observar. Utilizando Pandas, podemos utilizar o `pd.get_dummies`. Via modelagem statsmodels, basta dentro da formula ao ols, inserirmos como `outcome ~ C(Variable)`.

> [!tip]
>Lembrete: a regressão linear assume que a relação entre as regressoras e o resultado (condicional) é linear. Se a relação verdadeira for não linear, a especificação linear pode provocar viés de especificação. É importante verificar se isso ocorre, especialmente entre a variável de tratamento e o desfecho; caso ocorra, podemos aplicar transformações adequadas, por exemplo: logaritmos, termos polinomiais, interações ou transformações multiplicativas.

### Spline

No contexto de variáveis de controle com relação **não** linear, podemos modelar via Spline. Desta forma, o modelo consegue se convergir, conseguindo observar melhor a intervenção.

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.special import expit # Equivalente ao plogis (inverse logit)
import matplotlib.pyplot as plt

# 1. Funções de Simulação ----
def sim_data(n=250, beta_trt=1.5, z1_mean=5, z1_sd=2, 
             z2_size=1, z2_prob=0.5, z1_on_x=0.05, 
             z2_on_x=0.2, z1_on_y=0.5, z2_on_y=0.3):
    
    # Criando o DataFrame
    z1 = np.random.normal(z1_mean, z1_sd, n)
    z2 = np.random.binomial(z2_size, z2_prob, n)
    
    prob = expit(z1_on_x * z1 + z2_on_x * z2)
    x = np.random.binomial(1, prob, n)
    
    # Gerando Y (Linear por padrão, vamos alterar depois)
    y = beta_trt * x + z1_on_y * z1 + z2_on_y * z2 + np.random.normal(0, 1, n)
    
    return pd.DataFrame({'x': x, 'y': y, 'z1': z1, 'z2': z2})

# 2. Gerando Dados Não-Lineares ----
np.random.seed(456)
df = sim_data()
# Alterando y para ter relação não-linear com z1 (z1^3 + z1)
df['y'] = 1.5 * df['x'] + (df['z1']**3) + df['z1'] + df['z2'] + np.random.normal(0, 1, 250)

# 3. Ajustando Modelos ----

# Modelo 1: Apenas termos lineares
mod1 = smf.ols("y ~ x + z1 + z2", data=df).fit()
print("--- Modelo 1: Linear ---")
print(mod1.summary().tables[1])

# Modelo 2: Usando Natural Splines (cr() ou bs())
# O patsy usa cr() para cubics splines ou você pode importar splines. 
# Para Natural Splines igual ao R, usamos 'cr' ou instalamos dmetar/patsy extensões.
# O termo cr(z1, df=4) é o mais próximo do ns(z1, 4)
mod2 = smf.ols("y ~ x + cr(z1, df=4) + z2", data=df).fit()
print("\n--- Modelo 2: Splines ---")
print(mod2.summary().tables[1])

# 4. Verificação de Performance (Bonus) ----
# Python não tem uma biblioteca idêntica ao 'performance' do R em um único comando,
# mas podemos checar os resíduos manualmente.

def check_predictions(model, title):
    plt.figure(figsize=(8, 5))
    plt.hist(model.model.endog, alpha=0.5, label='Realidade', bins=30)
    plt.hist(model.fittedvalues, alpha=0.5, label='Predição', bins=30)
    plt.title(f"Check Predictions: {title}")
    plt.legend()
    plt.show()

check_predictions(mod1, "Modelo Linear")
check_predictions(mod2, "Modelo com Splines")
```
# Regressão Linear como Modelo para Potenciais Outcome

Outra possibilidade de utilizar a regressão linear, é atuar como um modelo de imputação de potenciais resultados. Isto quer dizer que conseguimos estimar os efeitos causais, seja ATE ou ATT, preechendo valores contrafactuais.
A lógica reside na capacidade da regressão de modelar as funções de resultado potencial:$E[Y_0|X]$ e $E[Y_1|X]$.

## Efeito de Tratamento Médio (ATE)

> [!tip] 
> Relembrando! O $ATE$ é calculado como a diferença média entre o que toda a população teria se fosse tratada $\hat{E}[Y_1 | X_i]$ e o que toda a população teria se não fosse tratada $\hat{E}[Y_0 | X_i]$
> - $ATE = \frac{1}{N} \sum_{i} (\hat{E}[Y_1 | X_i] - \hat{E}[Y_0 | X_i])$
> 
> Onde $\hat{E}[Y_0 | X_i]$ e $\hat{E}[Y_1 | X_i]$ são modelos de regressão ajustados, respectivamente, nas unidades de controle $T=0$ e nas unidades tratadas $T=1$.

### Cálculo Simplificado com `statsmodels`

Em um modelo de regressão linear que inclui covariáveis $X$, o estimador do $ATE$ é equivalente ao coeficiente da variável de tratamento $D$.

Se o seu modelo é $Y = \beta_0 + \beta_1 D + \beta_2 X + \epsilon$, a estimativa de $\hat{\beta}_1$ é o $ATE$.

Python

```python
formula_ate = 'Y ~ D + X1 + X2'
model_ate = smf.ols(formula_ate, data=data).fit()
ate_estimate = model_ate.params['D']
```

## Efeito Médio de Tratamento nos Tratados (ATT)

> [!tip]
> Relembrando! O $ATT$ é a diferença média entre o resultado **observado** para o grupo tratado $Y_i$ e o seu resultado contrafactual imputado $\hat{E}[Y_0 | X_i]$
> $ATT = \frac{1}{N_1} \sum_{i: D_i=1} (Y_i - \hat{E}[Y_0 | X_i])$
> Isto significa que usamos o grupo de controle $D=0$ para construir o modelo que prevê o resultado potencial $Y_0$.

### Cálculo Simplificado com `statsmodels`

```python
model_mu0 = smf.ols('Y ~ X1 + X2', data=data[data['D'] == 0]).fit()
imputed_y0 = model_mu0.predict(data[data['D'] == 1]) # Imputar o contrafactual, isto é, usar model_mu0 para prever o Y_0 para as unidades do grupo tratado T=1. 
att_estimate = (data[data['D'] == 1]['Y'] - imputed_y0).mean()
```

