---
{"dg-publish":true,"permalink":"/digital-garden/path-for-the-true-and-brave/text/7-regressao-linear/"}
---

# RegressÃ£o Linear

Modelos de regressÃ£o linear tÃªm ampla aplicabilidade na inferÃªncia causal. Pelo mÃ©todo dos mÃ­nimos quadrados ordinÃ¡rios (MQO), conseguimos estimar parÃ¢metros que, sob suposiÃ§Ãµes apropriadas, tÃªm interpretaÃ§Ã£o causal. 

Dada uma relaÃ§Ã£o simples Y ~ X + Îµ, podemos estimar o efeito de X sobre Y pelas estimativas MQO, $\hat{Î²}_0$ e $\hat{Î²}_1$.

> [!tip]
> O coeficiente $\hat{Î²}_0$ pode ser interpretado como o valor esperado de $Y$ quando $X = 0$ (e quando todas as outras variÃ¡veis no modelo sÃ£o zero). 
> - AtenÃ§Ã£o: essa interpretaÃ§Ã£o sÃ³ Ã© Ãºtil quando X = 0 Ã© um valor plausÃ­vel/observado.

> [!tip]
> Em um estudo com grupo de controle e tratamento (X indica tratamento), $\hat{Î²}_0$ Ã© a mÃ©dia do grupo controle e $\hat{Î²}_1$ Ã© a diferenÃ§a mÃ©dia (efeito mÃ©dio) entre tratamento e controle â€” desde que as suposiÃ§Ãµes de identificaÃ§Ã£o sejam satisfeitas.

O coeficiente $\hat{Î²}_1$ representa a variaÃ§Ã£o mÃ©dia esperada em $Y$ resultante de um aumento unitÃ¡rio em $X$, mantendo constantes as demais covariÃ¡veis do modelo.
- Se $X$ for a variÃ¡vel de tratamento e for contÃ­nua, $\hat{Î²}_1$ Ã© a variaÃ§Ã£o mÃ©dia de $Y$ associada a um aumento unitÃ¡rio em $X$ (sob suposiÃ§Ãµes causais).
- Se $X$ for uma dummy de tratamento e as suposiÃ§Ãµes de identificaÃ§Ã£o forem plausÃ­veis, $\hat{Î²}_1$ pode ser interpretado como o Efeito MÃ©dio do Tratamento (ATE ou ATT dependendo da populaÃ§Ã£o condicionada). Se $Y$ for binÃ¡rio e vocÃª usar um modelo linear de probabilidade, $\hat{Î²}_1$ aproxima o efeito mÃ©dio (ATE) sob ignorabilidade, mas tem limitaÃ§Ãµes (valores previstos fora de [0,1], heterocedasticidade). Para resultados binÃ¡rios, usar regressÃ£o logÃ­stica.

> [!tip] ðŸ’¡
> Ã‰ crucial controlar variÃ¡veis de confusÃ£o. Se omitirmos variÃ¡veis que influenciam tanto $X$ quanto $Y$, seu efeito serÃ¡ absorvido no termo de erro e incorretamente atribuÃ­do a $X$, gerando viÃ©s de variÃ¡vel omitida. Em caso de dÃºvida sobre quais tipos de variÃ¡veis vale a pena controlar, veja este paper, se baseando no DAG: [A Crash Course in Good and Bad Controls](https://ftp.cs.ucla.edu/pub/stat_ser/r493.pdf)

# O Trade-off entre ViÃ©s e VariÃ¢ncia

Entretanto, condicionar (ou incluir) covariÃ¡veis irrelevantes ou em excesso no modelo pode introduzir um risco: o aumento da variÃ¢ncia do estimador de $\hat{\beta}_1$.
O aumento na variÃ¢ncia ocorre frequentemente devido Ã  multicolinearidade e significa que as estimativas de $\hat{\beta}_1$ serÃ£o menos precisas. Isso resulta em intervalos de confianÃ§a mais amplo, tornando mais difÃ­cil rejeitar a hipÃ³tese nula e obter um resultado estatisticamente significativo.
## Teorema Frischâ€‘Waughâ€‘Lovell

Outra forma de observamos isso, Ã© entendermos melhor o funcionamento do teorema Teorema Frisch-Waugh-Lovell (FWL).

Ele demonstra que o coeficiente de $X$ em uma regressÃ£o mÃºltipla Y ~ X + Z Ã© igual ao coeficiente obtido ao regressar os resÃ­duos de $Y$ sobre $Z$ nos resÃ­duos de $X$ sobre $Z$. Em termos prÃ¡ticos, isso significa que a contribuiÃ§Ã£o de cada regressora pode ser entendida como a associaÃ§Ã£o entre as partes de $Y$ e $X$ que nÃ£o sÃ£o explicadas por $Z$. Isto pode ser dividido em trÃªs etapas

1. **Desviesamento (*****Debiasing Step*****)**
	- Remover o viÃ©s das variÃ¡veis de controle $Z$ da sua variÃ¡vel de interesse $X$.
    - **Regredimos** a variÃ¡vel de tratamento $X$ nas covariÃ¡veis de controle $Z$, (ex: $X \sim Z$)
    - Coletamos os **resÃ­duos** dessa regressÃ£o $\tilde{X}$. Estes resÃ­duos representam a parte de $X$ que Ã© **ortogonal** (nÃ£o correlacionada) aos controles $Z$.
2. **RemoÃ§Ã£o de RuÃ­do (*****Denoising Step*****)**
	- Remover o ruÃ­do das variÃ¡veis de controle $Z$ da sua variÃ¡vel dependente $Y$.
    - Regredimos a variÃ¡vel dependente $Y$ nas mesmas covariÃ¡veis de controle $Z$, (ex: $Y \sim Z$)
	- Coletamos os **resÃ­duos** dessa regressÃ£o $\tilde{Y}$. Estes resÃ­duos representam a parte de $Y$ que Ã© **independente** dos controles $Z$.
 3. **RegressÃ£o Final**
	- A estimativa final do efeito causal $\hat{\beta}_1$ Ã© obtida regredindo o resÃ­duo da variÃ¡vel dependente no resÃ­duo da variÃ¡vel de tratamento: $\tilde{Y} \sim \tilde{X}$.
	- O $\hat{\beta}$ obtido nesta regressÃ£o de resÃ­duos Ã© **idÃªntico** ao $\hat{\beta}_1$ da regressÃ£o original $Y \sim X + Z$. 

Uma melhor explicaÃ§Ã£o sobre este tema pode ser encontrada no livro [Causal Inference in Python: Applying Causal Inference in the Tech Industry, de Matheus Facure](https://www.amazon.com/Causal-Inference-Python-Applying-Industry/dp/1098140257).

Para ver os exemplos via cÃ³digo, [Ã© sÃ³ acessar aqui.](https://github.com/matheusfacure/causal-inference-in-python-code)

> [!tip]
> Vale relembrarmos, que para atuaÃ§Ã£o de dados categoricos - ou discretizaÃ§Ãµes de valores nÃºmericos que queremos atuar como - necessitamos transformÃ¡-los como variÃ¡vel Dummy. Para cada coluna, Ã© necessÃ¡rio transformÃ¡-lo em uma covariÃ¡vel de 0 ou 1. Assim, o modelo de RegressÃ£o irÃ¡ comportar cada categoria como uma linha adversa, se comportando como uma categoria a parte ao outcome que queremos observar. Utilizando Pandas, podemos utilizar o `pd.get_dummies`. Via modelagem statsmodels, basta dentro da formula ao ols, inserirmos como `outcome ~ C(Variable)`.

> [!tip]
>Lembrete: a regressÃ£o linear assume que a relaÃ§Ã£o entre as regressoras e o resultado (condicional) Ã© linear. Se a relaÃ§Ã£o verdadeira for nÃ£o linear, a especificaÃ§Ã£o linear pode provocar viÃ©s de especificaÃ§Ã£o. Ã‰ importante verificar se isso ocorre, especialmente entre a variÃ¡vel de tratamento e o desfecho; caso ocorra, podemos aplicar transformaÃ§Ãµes adequadas, por exemplo: logaritmos, termos polinomiais, interaÃ§Ãµes ou transformaÃ§Ãµes multiplicativas.

### Spline

No contexto de variÃ¡veis de controle com relaÃ§Ã£o **nÃ£o** linear, podemos modelar via Spline. Desta forma, o modelo consegue se convergir, conseguindo observar melhor a intervenÃ§Ã£o.

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.special import expit # Equivalente ao plogis (inverse logit)
import matplotlib.pyplot as plt

# 1. FunÃ§Ãµes de SimulaÃ§Ã£o ----
def sim_data(n=250, beta_trt=1.5, z1_mean=5, z1_sd=2, 
             z2_size=1, z2_prob=0.5, z1_on_x=0.05, 
             z2_on_x=0.2, z1_on_y=0.5, z2_on_y=0.3):
    
    # Criando o DataFrame
    z1 = np.random.normal(z1_mean, z1_sd, n)
    z2 = np.random.binomial(z2_size, z2_prob, n)
    
    prob = expit(z1_on_x * z1 + z2_on_x * z2)
    x = np.random.binomial(1, prob, n)
    
    # Gerando Y (Linear por padrÃ£o, vamos alterar depois)
    y = beta_trt * x + z1_on_y * z1 + z2_on_y * z2 + np.random.normal(0, 1, n)
    
    return pd.DataFrame({'x': x, 'y': y, 'z1': z1, 'z2': z2})

# 2. Gerando Dados NÃ£o-Lineares ----
np.random.seed(456)
df = sim_data()
# Alterando y para ter relaÃ§Ã£o nÃ£o-linear com z1 (z1^3 + z1)
df['y'] = 1.5 * df['x'] + (df['z1']**3) + df['z1'] + df['z2'] + np.random.normal(0, 1, 250)

# 3. Ajustando Modelos ----

# Modelo 1: Apenas termos lineares
mod1 = smf.ols("y ~ x + z1 + z2", data=df).fit()
print("--- Modelo 1: Linear ---")
print(mod1.summary().tables[1])

# Modelo 2: Usando Natural Splines (cr() ou bs())
# O patsy usa cr() para cubics splines ou vocÃª pode importar splines. 
# Para Natural Splines igual ao R, usamos 'cr' ou instalamos dmetar/patsy extensÃµes.
# O termo cr(z1, df=4) Ã© o mais prÃ³ximo do ns(z1, 4)
mod2 = smf.ols("y ~ x + cr(z1, df=4) + z2", data=df).fit()
print("\n--- Modelo 2: Splines ---")
print(mod2.summary().tables[1])

# 4. VerificaÃ§Ã£o de Performance (Bonus) ----
# Python nÃ£o tem uma biblioteca idÃªntica ao 'performance' do R em um Ãºnico comando,
# mas podemos checar os resÃ­duos manualmente.

def check_predictions(model, title):
    plt.figure(figsize=(8, 5))
    plt.hist(model.model.endog, alpha=0.5, label='Realidade', bins=30)
    plt.hist(model.fittedvalues, alpha=0.5, label='PrediÃ§Ã£o', bins=30)
    plt.title(f"Check Predictions: {title}")
    plt.legend()
    plt.show()

check_predictions(mod1, "Modelo Linear")
check_predictions(mod2, "Modelo com Splines")
```
# RegressÃ£o Linear como Modelo para Potenciais Outcome

Outra possibilidade de utilizar a regressÃ£o linear, Ã© atuar como um modelo de imputaÃ§Ã£o de potenciais resultados. Isto quer dizer que conseguimos estimar os efeitos causais, seja ATE ou ATT, preechendo valores contrafactuais.
A lÃ³gica reside na capacidade da regressÃ£o de modelar as funÃ§Ãµes de resultado potencial:$E[Y_0|X]$ e $E[Y_1|X]$.

## Efeito de Tratamento MÃ©dio (ATE)

> [!tip] 
> Relembrando! O $ATE$ Ã© calculado como a diferenÃ§a mÃ©dia entre o que toda a populaÃ§Ã£o teria se fosse tratada $\hat{E}[Y_1 | X_i]$ e o que toda a populaÃ§Ã£o teria se nÃ£o fosse tratada $\hat{E}[Y_0 | X_i]$
> - $ATE = \frac{1}{N} \sum_{i} (\hat{E}[Y_1 | X_i] - \hat{E}[Y_0 | X_i])$
> 
> Onde $\hat{E}[Y_0 | X_i]$ e $\hat{E}[Y_1 | X_i]$ sÃ£o modelos de regressÃ£o ajustados, respectivamente, nas unidades de controle $T=0$ e nas unidades tratadas $T=1$.

### CÃ¡lculo Simplificado com `statsmodels`

Em um modelo de regressÃ£o linear que inclui covariÃ¡veis $X$, o estimador do $ATE$ Ã© equivalente ao coeficiente da variÃ¡vel de tratamento $T$.

Se o seu modelo Ã© $Y = \beta_0 + \beta_1 T + \beta_2 X + \epsilon$, a estimativa de $\hat{\beta}_1$ Ã© o $ATE$.

Python

```javascript
formula_ate = 'Y ~ T + X1 + X2'
model_ate = smf.ols(formula_ate, data=data).fit()
ate_estimate = model_ate.params['T']
```

## Efeito MÃ©dio de Tratamento nos Tratados (ATT)

> [!tip] ðŸ’¡
> Relembrando! O $ATT$ Ã© a diferenÃ§a mÃ©dia entre o resultado **observado** para o grupo tratado $Y_i$ e o seu resultado contrafactual imputado $\hat{E}[Y_0 | X_i]$
> $ATT = \frac{1}{N_1} \sum_{i: T_i=1} (Y_i - \hat{E}[Y_0 | X_i])$
> Isto significa que usamos o grupo de controle $T=0$ para construir o modelo que prevÃª o resultado potencial $Y_0$.

### CÃ¡lculo Simplificado com `statsmodels`

```python
model_mu0 = smf.ols('Y ~ X1 + X2', data=data[data['T'] == 0]).fit()
imputed_y0 = model_mu0.predict(data[data['T'] == 1]) # Imputar o contrafactual, isto Ã©, usar model_mu0 para prever o Y_0 para as unidades do grupo tratado T=1. 
att_estimate = (data[data['T'] == 1]['Y'] - imputed_y0).mean()
```

